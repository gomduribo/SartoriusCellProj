{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73dea87-08a4-4601-94c5-bd71c7f94b5e",
   "metadata": {},
   "source": [
    "### 1. 모델 구현\n",
    "### 2. 데이터 클래스\n",
    "### 3. 데이터 로더\n",
    "### 4. 손실함수\n",
    "### 5. optimizer\n",
    "### 6. weight 저장후, inference\n",
    "### 7. 평가지표로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd85c93d-7928-4515-babd-711c88a6fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. 모델 구현 OK\n",
    "# 직사각형 input, 직사각형 output 되도록 kernel사이즈나 그런것들 조절해주기\n",
    "### 2. 데이터 클래스 @\n",
    "#  run length encoded된거 decode하기 @\n",
    "### 3. 데이터 로더 @\n",
    "### 4. Transformer, collate_fn 구현 -----> Proceeding...\n",
    "# 데이터 클래스 이전에 구현해서 데이터 인스턴스에 넣어주기\n",
    "### 5. 손실함수\n",
    "### 6. optimizer\n",
    "### 7. weight 저장후, inference\n",
    "### 8. 평가지표로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bfcc53-0d4c-486a-b41c-c151fb673221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa45a44-3048-4108-afc3-c3b7e2472c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.모델구현\n",
    "class Unet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.ConPathLayer1 = nn.Sequential(\n",
    "        # torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(1,3), stride=1, padding=(25,0), padding_mode='reflect', dilation=(1,67)),\n",
    "        torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.ConPathLayer2 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.ConPathLayer3 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.ConPathLayer4 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool4 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.BottomLayer = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv1 = torch.nn.ConvTranspose2d(in_channels=1024, out_channels=512,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer1 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv2 = torch.nn.ConvTranspose2d(in_channels=512, out_channels=256,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer2 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv3 = torch.nn.ConvTranspose2d(in_channels=256, out_channels=128,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer3 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv4 = torch.nn.ConvTranspose2d(in_channels=128, out_channels=64,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer4 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1, stride=1)\n",
    "        # torch.nn.Conv2d(in_channels=64, out_channels=2, kernel_size=(1,1), stride=1, padding=(66,158), padding_mode='replicate', dilation=(1,1))\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.ConPathLayer1(x)\n",
    "    skip1 = torchvision.transforms.CenterCrop(392)(x)\n",
    "    x = self.MaxPool1(x)\n",
    "    x = self.ConPathLayer2(x)\n",
    "    skip2 = torchvision.transforms.CenterCrop(200)(x)\n",
    "    x = self.MaxPool2(x)\n",
    "    x = self.ConPathLayer3(x)\n",
    "    skip3 = torchvision.transforms.CenterCrop(104)(x)\n",
    "    x = self.MaxPool3(x)\n",
    "    x = self.ConPathLayer4(x)\n",
    "    skip4 = torchvision.transforms.CenterCrop(56)(x)\n",
    "    x = self.MaxPool4(x)\n",
    "    # print(\"skip1:{}, skip2:{}, skip3:{}, skip4:{}\".format(skip1.shape, skip2.shape, skip3.shape, skip4.shape))\n",
    "    x = self.BottomLayer(x)\n",
    "    x = self.UpConv1(x)\n",
    "    x = torch.cat((skip4, x), dim=1)\n",
    "    x = self.ExpPathLayer1(x)\n",
    "    x = self.UpConv2(x)\n",
    "    x = torch.cat((skip3, x), dim=1)\n",
    "    x = self.ExpPathLayer2(x)\n",
    "    x = self.UpConv3(x)\n",
    "    x = torch.cat((skip2, x), dim=1)\n",
    "    x = self.ExpPathLayer3(x)\n",
    "    x = self.UpConv4(x)\n",
    "    x = torch.cat((skip1, x), dim=1)\n",
    "    result = self.ExpPathLayer4(x)\n",
    "\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74bd23fd-79da-4485-be34-25fb697da161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 데이터 클래스 정의 \n",
    "class CT_Dataset():\n",
    "    def __init__(self, phase, transformer=None,target_transformer=None):\n",
    "        # self.train_csv_path = \"C:/Users/syb62/Desktop/Unet/train/train.csv\"\n",
    "        # self.train_path = \"C:/Users/syb62/Desktop/Unet/train\"\n",
    "        self.train_csv_path = \"/home/inbic/Desktop/Unet/train/train.csv\"\n",
    "        self.train_path = \"/home/inbic/Desktop/Unet/train\"\n",
    "        \n",
    "        self.phase = phase\n",
    "        self.train_img_list = [] # train 이미지들 목록\n",
    "        self.image_info = collections.defaultdict(dict) # train이미지 id, 이미지 경로, 라벨(annotation)\n",
    "        self.transformer = transformer\n",
    "        self.target_transformer = target_transformer\n",
    "        \n",
    "        ################ train이미지 목록 리스트 생성 ################\n",
    "        train_list = os.listdir(self.train_path)\n",
    "        for i in train_list:\n",
    "          if i.endswith(\".png\"):\n",
    "            self.train_img_list.append(i)\n",
    "\n",
    "        ################ image_info 생성 ################\n",
    "        train_df = pd.read_csv(self.train_csv_path)\n",
    "        anno_list=[]\n",
    "        df_ids = np.array(train_df.loc[:, \"id\"].values).tolist()\n",
    "        mask_dic = {}\n",
    "\n",
    "        for img_name in self.train_img_list:\n",
    "          id = img_name.split(\".\")[0]\n",
    "          anno_list=[]\n",
    "          for j in range(len(df_ids)):\n",
    "            if df_ids[j] == id:\n",
    "              a = train_df.loc[:, \"annotation\"].values[j]\n",
    "              anno_list.append(a)\n",
    "            else:\n",
    "              pass\n",
    "          mask_dic[id]=anno_list\n",
    "\n",
    "          for i in range(len(mask_dic.keys())):\n",
    "            mask_dic_keys = list(mask_dic.keys())\n",
    "            self.image_info[i]={\n",
    "                    'image_id': mask_dic_keys[i],\n",
    "                    'image_path': os.path.join(self.train_path, mask_dic_keys[i] + '.png'),\n",
    "                    'annotations': mask_dic[mask_dic_keys[i]]\n",
    "                    }\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.train_img_list)\n",
    "    \n",
    "    def rle_decode(self, mask_rle, shape, color=1):\n",
    "        '''\n",
    "        mask_rle: run-length as string formated (start length)\n",
    "        shape: (height,width) of array to return \n",
    "        Returns numpy array, 1 - mask, 0 - background\n",
    "        '''\n",
    "        s = mask_rle.split()\n",
    "        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "        starts -= 1\n",
    "        ends = starts + lengths\n",
    "        img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n",
    "        for lo, hi in zip(starts, ends):\n",
    "            img[lo : hi] = color\n",
    "        return img.reshape(shape)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        info = self.image_info[idx]\n",
    "        img_path = info[\"image_path\"]\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        ##########################\n",
    "\n",
    "        HEIGHT = 520\n",
    "        WIDTH = 704\n",
    "        # HEIGHT = 388\n",
    "        # WIDTH = 388\n",
    "        height = HEIGHT\n",
    "        width = WIDTH\n",
    "\n",
    "        masks = np.zeros((len(info['annotations']), height, width), dtype=np.uint8)\n",
    "        labels=[]\n",
    "\n",
    "        for i, annotation in enumerate(info['annotations']):\n",
    "            a_mask = self.rle_decode(annotation, (HEIGHT, WIDTH))\n",
    "            a_mask = Image.fromarray(a_mask)\n",
    "\n",
    "            # if self.should_resize:\n",
    "            if True:\n",
    "                a_mask = a_mask.resize((width, height), resample=Image.BILINEAR)\n",
    "                a_mask = np.array(a_mask) > 0\n",
    "                labels.append(a_mask)\n",
    "                masks[i, :, :] = a_mask\n",
    "        ##########################\n",
    "        total = np.zeros((520, 704))\n",
    "        # total = np.zeros((388, 388))\n",
    "        for i in range(len(labels)):\n",
    "            total = total + labels[i]\n",
    "        \n",
    "        # total= (total>=1)\n",
    "        target = (total>=1)\n",
    "        target_reverse = (target==0)\n",
    "        # result = np.zeros((2,520,704))\n",
    "        # result[0,:,:]=target\n",
    "        # result[1,:,:]=target_reverse\n",
    "        if self.transformer:\n",
    "            image = self.transformer(image)\n",
    "\n",
    "        if self.target_transformer:\n",
    "            target_trans = self.target_transformer(target)\n",
    "\n",
    "            # target_trans1 = self.target_transformer(target)\n",
    "            # target_trans2 = self.target_transformer(target_reverse)\n",
    "        \n",
    "        # target = torch.from_numpy(target).long()\n",
    "\n",
    "        # 388, 388 출력\n",
    "        print(target_trans.shape)\n",
    "        # target = target_trans[0,:,:].long()\n",
    "        target = target_trans[0,:,:].long()\n",
    "        \n",
    "\n",
    "        # 2,388, 388 출력\n",
    "        # target = torch.zeros([2,388,388])\n",
    "        # target[0,:,:] = target_trans1\n",
    "        # target[1,:,:] = target_trans2\n",
    "        \n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "445ca2b8-3112-4bfd-b571-ff66e49ecd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Transformer & collate_fn 구현\n",
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(572,572))\n",
    "    # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    \n",
    "])\n",
    "\n",
    "target_transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(388,388))\n",
    "    # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc9b282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 388, 388])\n",
      "torch.Size([1, 388, 388])\n"
     ]
    }
   ],
   "source": [
    "input = np.random.randn(1, 3, 572, 572)\n",
    "input = torch.tensor(input).float()\n",
    "model = Unet()\n",
    "prediction = model(input)\n",
    "print(prediction.shape)\n",
    "predictions_ = torch.argmax(prediction, dim=1)\n",
    "print(predictions_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f1b7c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 388, 388])\n",
      "torch.Size([388, 388])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inbic/anaconda3/envs/unet/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = CT_Dataset(phase='train', transformer=transformer, target_transformer=target_transformer)\n",
    "image, target = data[10]\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d242b57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9b734de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(target, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb69e02f-1b2f-48b2-813c-7c947e02c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for a, b in batch: \n",
    "        images.append(a)\n",
    "        targets.append(b)\n",
    "    images = torch.stack(images, dim=0) \n",
    "    targets = torch.stack(targets, dim=0)\n",
    "\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b8a05f1-5f87-4b19-af36-a582855f8dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. 손실함수 계산 Class 선언\n",
    "class Unet_loss():\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes=num_classes\n",
    "\n",
    "        self.CE_loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        self.BCE_loss = nn.BCELoss()\n",
    "        self.BCELogit_loss = nn.BCEWithLogitsLoss()\n",
    "        # [B, 2, 520, 704] 크기의 prediction입력\n",
    "        # self.prediction=prediction\n",
    "        # [B, 520, 704] 크기의 target입력\n",
    "        # self.target=target\n",
    "    def __call__(self, prediction, target):\n",
    "        loss = self.get_loss(prediction, target)\n",
    "        return loss\n",
    "    \n",
    "    def get_loss(self,  prediction, target):\n",
    "        ################ Dice Coefficient 적용할거면 연산 진행해서 넣어주기 ################\n",
    "        # prediction: [B, 2, 520, 704] -> [B, 2, 520, 704] 예측한 label값으로 변환\n",
    "        # predictions_ = torch.argmax(prediction, dim=1)\n",
    "        # onehot_pred = F.one_hot(predictions_,num_classes=self.num_classes)\n",
    "        # onehot_pred = onehot_pred.permute(0,3,1,2)\n",
    "        \n",
    "        # prediction: [B, 520, 704] -> [B, 2, 520, 704] 을 예측한 label값으로 변환\n",
    "        # onehot_target = F.one_hot(target , num_classes=self.num_classes).permute(0,3,1,2)\n",
    "        ########################################################################################\n",
    "        \n",
    "        # CE loss 계산\n",
    "        # ce_loss = self.CE_loss(onehot_pred, onehot_target)\n",
    "        predictions_ = torch.argmax(prediction, dim=1)\n",
    "\n",
    "        # ce_loss = self.CE_loss(predictions_, target)\n",
    "        # ce_loss = self.CE_loss(prediction, target)\n",
    "        # bce_loss = self.BCE_loss(predictions_,target)\n",
    "        bcelogit_loss = self.BCELogit_loss(predictions_,target)\n",
    "        \n",
    "        return bcelogit_loss\n",
    "        # return bce_loss\n",
    "        # return ce_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c2d6f91-9f3a-41c1-aa82-8d46d06be89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Training\n",
    "def train_one_epoch(trainloader, model,optimizer,criterion,device):\n",
    "    losses = {}\n",
    "    # for phase in ['train', 'val']:\n",
    "    for phase in ['train']:\n",
    "        running_loss = 0.0\n",
    "\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        for idx, batch in enumerate(trainloader):\n",
    "            train_batch_data = batch[0].to(device)\n",
    "            target_batch = batch[1].to(device)\n",
    "            \n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                pred = model(train_batch_data)\n",
    "                print(f\"pred:{pred.dtype()}, target:{target_batch.dtype()}\")\n",
    "                loss = criterion(pred,target_batch.float())\n",
    "            \n",
    "            if phase == 'train':\n",
    "                optimizer.zero_grad() \n",
    "                # optimizer의 갱신할 기울기 값을, 즉, 연결되어있는 모든 model params의 기울기값들을 초기화\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss = loss.item()\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                if idx % 5 == 0:\n",
    "                    text = f\"{idx}/{len(trainloader)}\" + \\\n",
    "                            f\" - Running Loss: {loss.item():.4f}\"\n",
    "                    print(text)\n",
    "            \n",
    "        losses[phase] = running_loss / len(trainloader)\n",
    "                    \n",
    "    # print(f\"idx:{idx}, batch image: {train_batch_data.shape}, \\\n",
    "    # batch target:{target_batch.shape}, output: {pred.shape}, loss: {loss}\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9db5a28-930f-4bd6-aa3e-e22c4d56af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_state, model_name, save_dir=\"/home/inbic/Desktop/SartoriusCellProj/models_1050_square_2_1_BCE\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model_state, os.path.join(save_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ba5e42a-75bd-45cc-b78f-8263d9d7eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = True\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available and is_cuda else 'cpu')\n",
    "# DEVICE = torch.device('cpu')\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 20\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "train_data = CT_Dataset(phase='train', transformer=transformer, target_transformer=target_transformer)\n",
    "trainloader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn,shuffle=True)\n",
    "model = Unet()\n",
    "model = model.to(DEVICE)\n",
    "criterion = Unet_loss(num_classes=NUM_CLASSES)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= LEARNING_RATE, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68e5303b-3f0b-40c6-9b34-dbc57ae601e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 388, 388])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inbic/anaconda3/envs/unet/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:9\u001b[0m\n",
      "Cell \u001b[0;32mIn[24], line 19\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(trainloader, model, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(phase\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     18\u001b[0m     pred \u001b[39m=\u001b[39m model(train_batch_data)\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpred:\u001b[39m\u001b[39m{\u001b[39;00mpredc\u001b[39m}\u001b[39;00m\u001b[39m, target:\u001b[39m\u001b[39m{\u001b[39;00mtarget_batch\u001b[39m.\u001b[39mdtype()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     loss \u001b[39m=\u001b[39m criterion(pred,target_batch\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predc' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = EPOCHS\n",
    "\n",
    "best_epoch = 0\n",
    "best_score = 0.0\n",
    "train_loss = []\n",
    "# val_loss, val_dice_coefficient = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = train_one_epoch(trainloader, model, optimizer, criterion, DEVICE)\n",
    "    train_loss.append(losses[\"train\"])\n",
    "    # val_loss.append(losses[\"val\"])\n",
    "    \n",
    "    print(f\"{epoch}/{num_epochs} - Train Loss: {losses['train']:.4f}\")\n",
    "    \n",
    "    if epoch % 2 ==0:\n",
    "        save_model(model.state_dict(), f\"model_{epoch:02d}.pth\")\n",
    "        \n",
    "print(f\"Best epoch: {best_epoch} -> Best Dice Coeffient: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88837590-f157-4479-9ef0-08ec7e27a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. weight 저장후, inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee27ead-84fa-4902-a91a-095189fd055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9. 평가지표로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807919ee-3355-4cf2-a1f9-cf8e98ed2bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f9a62-2684-4cb7-963e-1e49a4674a25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
