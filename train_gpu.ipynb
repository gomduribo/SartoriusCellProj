{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73dea87-08a4-4601-94c5-bd71c7f94b5e",
   "metadata": {},
   "source": [
    "### 1. 모델 구현\n",
    "### 2. 데이터 클래스\n",
    "### 3. 데이터 로더\n",
    "### 4. 손실함수\n",
    "### 5. optimizer\n",
    "### 6. weight 저장후, inference\n",
    "### 7. 평가지표로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd85c93d-7928-4515-babd-711c88a6fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. 모델 구현 OK\n",
    "# 직사각형 input, 직사각형 output 되도록 kernel사이즈나 그런것들 조절해주기\n",
    "### 2. 데이터 클래스 @\n",
    "#  run length encoded된거 decode하기 @\n",
    "### 3. 데이터 로더 @\n",
    "### 4. Transformer, collate_fn 구현 -----> Proceeding...\n",
    "# 데이터 클래스 이전에 구현해서 데이터 인스턴스에 넣어주기\n",
    "### 5. 손실함수\n",
    "### 6. optimizer\n",
    "### 7. weight 저장후, inference\n",
    "### 8. 평가지표로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59bfcc53-0d4c-486a-b41c-c151fb673221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa45a44-3048-4108-afc3-c3b7e2472c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.모델구현\n",
    "class Unet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.ConPathLayer1 = nn.Sequential(\n",
    "        # torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(1,3), stride=1, padding=(25,0), padding_mode='reflect', dilation=(1,67)),\n",
    "        torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.ConPathLayer2 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.ConPathLayer3 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.ConPathLayer4 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool4 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.BottomLayer = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv1 = torch.nn.ConvTranspose2d(in_channels=1024, out_channels=512,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer1 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv2 = torch.nn.ConvTranspose2d(in_channels=512, out_channels=256,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer2 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv3 = torch.nn.ConvTranspose2d(in_channels=256, out_channels=128,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer3 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv4 = torch.nn.ConvTranspose2d(in_channels=128, out_channels=64,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer4 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1, stride=1)\n",
    "        # torch.nn.Conv2d(in_channels=64, out_channels=2, kernel_size=(1,1), stride=1, padding=(66,158), padding_mode='replicate', dilation=(1,1))\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.ConPathLayer1(x)\n",
    "    skip1 = torchvision.transforms.CenterCrop(392)(x)\n",
    "    x = self.MaxPool1(x)\n",
    "    x = self.ConPathLayer2(x)\n",
    "    skip2 = torchvision.transforms.CenterCrop(200)(x)\n",
    "    x = self.MaxPool2(x)\n",
    "    x = self.ConPathLayer3(x)\n",
    "    skip3 = torchvision.transforms.CenterCrop(104)(x)\n",
    "    x = self.MaxPool3(x)\n",
    "    x = self.ConPathLayer4(x)\n",
    "    skip4 = torchvision.transforms.CenterCrop(56)(x)\n",
    "    x = self.MaxPool4(x)\n",
    "    # print(\"skip1:{}, skip2:{}, skip3:{}, skip4:{}\".format(skip1.shape, skip2.shape, skip3.shape, skip4.shape))\n",
    "    x = self.BottomLayer(x)\n",
    "    x = self.UpConv1(x)\n",
    "    x = torch.cat((skip4, x), dim=1)\n",
    "    x = self.ExpPathLayer1(x)\n",
    "    x = self.UpConv2(x)\n",
    "    x = torch.cat((skip3, x), dim=1)\n",
    "    x = self.ExpPathLayer2(x)\n",
    "    x = self.UpConv3(x)\n",
    "    x = torch.cat((skip2, x), dim=1)\n",
    "    x = self.ExpPathLayer3(x)\n",
    "    x = self.UpConv4(x)\n",
    "    x = torch.cat((skip1, x), dim=1)\n",
    "    result = self.ExpPathLayer4(x)\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74bd23fd-79da-4485-be34-25fb697da161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 데이터 클래스 정의 \n",
    "class CT_Dataset():\n",
    "    def __init__(self, phase, transformer=None,target_transformer=None):\n",
    "        # self.train_csv_path = \"C:/Users/syb62/Desktop/Unet/train/train.csv\"\n",
    "        # self.train_path = \"C:/Users/syb62/Desktop/Unet/train\"\n",
    "        self.train_csv_path = \"/home/inbic/Desktop/Unet/train/train.csv\"\n",
    "        self.train_path = \"/home/inbic/Desktop/Unet/train\"\n",
    "        \n",
    "        self.phase = phase\n",
    "        self.train_img_list = [] # train 이미지들 목록\n",
    "        self.image_info = collections.defaultdict(dict) # train이미지 id, 이미지 경로, 라벨(annotation)\n",
    "        self.transformer = transformer\n",
    "        self.target_transformer = target_transformer\n",
    "        \n",
    "        ################ train이미지 목록 리스트 생성 ################\n",
    "        train_list = os.listdir(self.train_path)\n",
    "        for i in train_list:\n",
    "          if i.endswith(\".png\"):\n",
    "            self.train_img_list.append(i)\n",
    "\n",
    "        ################ image_info 생성 ################\n",
    "        train_df = pd.read_csv(self.train_csv_path)\n",
    "        anno_list=[]\n",
    "        df_ids = np.array(train_df.loc[:, \"id\"].values).tolist()\n",
    "        mask_dic = {}\n",
    "\n",
    "        for img_name in self.train_img_list:\n",
    "          id = img_name.split(\".\")[0]\n",
    "          anno_list=[]\n",
    "          for j in range(len(df_ids)):\n",
    "            if df_ids[j] == id:\n",
    "              a = train_df.loc[:, \"annotation\"].values[j]\n",
    "              anno_list.append(a)\n",
    "            else:\n",
    "              pass\n",
    "          mask_dic[id]=anno_list\n",
    "\n",
    "          for i in range(len(mask_dic.keys())):\n",
    "            mask_dic_keys = list(mask_dic.keys())\n",
    "            self.image_info[i]={\n",
    "                    'image_id': mask_dic_keys[i],\n",
    "                    'image_path': os.path.join(self.train_path, mask_dic_keys[i] + '.png'),\n",
    "                    'annotations': mask_dic[mask_dic_keys[i]]\n",
    "                    }\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.train_img_list)\n",
    "    \n",
    "    def rle_decode(self, mask_rle, shape, color=1):\n",
    "        '''\n",
    "        mask_rle: run-length as string formated (start length)\n",
    "        shape: (height,width) of array to return \n",
    "        Returns numpy array, 1 - mask, 0 - background\n",
    "        '''\n",
    "        s = mask_rle.split()\n",
    "        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "        starts -= 1\n",
    "        ends = starts + lengths\n",
    "        img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n",
    "        for lo, hi in zip(starts, ends):\n",
    "            img[lo : hi] = color\n",
    "        return img.reshape(shape)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        info = self.image_info[idx]\n",
    "        img_path = info[\"image_path\"]\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        ##########################\n",
    "\n",
    "        HEIGHT = 520\n",
    "        WIDTH = 704\n",
    "        # HEIGHT = 388\n",
    "        # WIDTH = 388\n",
    "        height = HEIGHT\n",
    "        width = WIDTH\n",
    "\n",
    "        masks = np.zeros((len(info['annotations']), height, width), dtype=np.uint8)\n",
    "        labels=[]\n",
    "\n",
    "        for i, annotation in enumerate(info['annotations']):\n",
    "            a_mask = self.rle_decode(annotation, (HEIGHT, WIDTH))\n",
    "            a_mask = Image.fromarray(a_mask)\n",
    "\n",
    "            # if self.should_resize:\n",
    "            if True:\n",
    "                a_mask = a_mask.resize((width, height), resample=Image.BILINEAR)\n",
    "                a_mask = np.array(a_mask) > 0\n",
    "                labels.append(a_mask)\n",
    "                masks[i, :, :] = a_mask\n",
    "        ##########################\n",
    "        total = np.zeros((520, 704))\n",
    "        # total = np.zeros((388, 388))\n",
    "        for i in range(len(labels)):\n",
    "            total = total + labels[i]\n",
    "        \n",
    "        total= (total>=1)\n",
    "        # total = total.reshape(((1, 520, 704)))\n",
    "        target = total\n",
    "        \n",
    "        if self.transformer:\n",
    "            image = self.transformer(image)\n",
    "\n",
    "        if self.target_transformer:\n",
    "            target_trans = self.target_transformer(target)\n",
    "        \n",
    "        # target = torch.from_numpy(target).long()\n",
    "        target = target_trans[0,:,:].long()\n",
    "        \n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445ca2b8-3112-4bfd-b571-ff66e49ecd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Transformer & collate_fn 구현\n",
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(572,572))\n",
    "    # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    \n",
    "])\n",
    "\n",
    "target_transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(388,388))\n",
    "    # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb69e02f-1b2f-48b2-813c-7c947e02c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for a, b in batch: \n",
    "        images.append(a)\n",
    "        targets.append(b)\n",
    "    images = torch.stack(images, dim=0) \n",
    "    targets = torch.stack(targets, dim=0)\n",
    "\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b8a05f1-5f87-4b19-af36-a582855f8dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. 손실함수 계산 Class 선언\n",
    "class Unet_loss():\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes=num_classes\n",
    "        self.CE_loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        # [B, 2, 520, 704] 크기의 prediction입력\n",
    "        # self.prediction=prediction\n",
    "        # [B, 520, 704] 크기의 target입력\n",
    "        # self.target=target\n",
    "    def __call__(self, prediction, target):\n",
    "        loss = self.get_loss(prediction, target)\n",
    "        return loss\n",
    "    \n",
    "    def get_loss(self,  prediction, target):\n",
    "        ################ Dice Coefficient 적용할거면 연산 진행해서 넣어주기 ################\n",
    "        # prediction: [B, 2, 520, 704] -> [B, 2, 520, 704] 예측한 label값으로 변환\n",
    "        predictions_ = torch.argmax(prediction, dim=1)\n",
    "        onehot_pred = F.one_hot(predictions_,num_classes=self.num_classes)\n",
    "        onehot_pred = onehot_pred.permute(0,3,1,2)\n",
    "        \n",
    "        # prediction: [B, 520, 704] -> [B, 2, 520, 704] 을 예측한 label값으로 변환\n",
    "        onehot_target = F.one_hot(target , num_classes=self.num_classes).permute(0,3,1,2)\n",
    "        ########################################################################################\n",
    "        \n",
    "        # CE loss 계산\n",
    "        # ce_loss = self.CE_loss(onehot_pred, onehot_target)\n",
    "        ce_loss = self.CE_loss(prediction, target)\n",
    "        \n",
    "        return ce_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c2d6f91-9f3a-41c1-aa82-8d46d06be89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Training\n",
    "def train_one_epoch(trainloader, model,optimizer,criterion,device):\n",
    "    losses = {}\n",
    "    # for phase in ['train', 'val']:\n",
    "    for phase in ['train']:\n",
    "        running_loss = 0.0\n",
    "\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        for idx, batch in enumerate(trainloader):\n",
    "            train_batch_data = batch[0].to(device)\n",
    "            target_batch = batch[1].to(device)\n",
    "            \n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                pred = model(train_batch_data)\n",
    "                loss = criterion(pred,target_batch)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                optimizer.zero_grad() \n",
    "                # optimizer의 갱신할 기울기 값을, 즉, 연결되어있는 모든 model params의 기울기값들을 초기화\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss = loss.item()\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                if idx % 5 == 0:\n",
    "                    text = f\"{idx}/{len(trainloader)}\" + \\\n",
    "                            f\" - Running Loss: {loss.item():.4f}\"\n",
    "                    print(text)\n",
    "            \n",
    "        losses[phase] = running_loss / len(trainloader)\n",
    "                    \n",
    "    # print(f\"idx:{idx}, batch image: {train_batch_data.shape}, \\\n",
    "    # batch target:{target_batch.shape}, output: {pred.shape}, loss: {loss}\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9db5a28-930f-4bd6-aa3e-e22c4d56af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_state, model_name, save_dir=\"/home/inbic/Desktop/SartoriusCellProj/models_1050_square\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model_state, os.path.join(save_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ba5e42a-75bd-45cc-b78f-8263d9d7eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = True\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available and is_cuda else 'cpu')\n",
    "# DEVICE = torch.device('cpu')\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 20\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "train_data = CT_Dataset(phase='train', transformer=transformer, target_transformer=target_transformer)\n",
    "trainloader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn,shuffle=True)\n",
    "model = Unet()\n",
    "model = model.to(DEVICE)\n",
    "criterion = Unet_loss(num_classes=NUM_CLASSES)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= LEARNING_RATE, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff4c7a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([388, 388])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inbic/anaconda3/envs/unet/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data, target = train_data[10]\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68e5303b-3f0b-40c6-9b34-dbc57ae601e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inbic/anaconda3/envs/unet/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/606 - Running Loss: 0.6574\n",
      "5/606 - Running Loss: 0.6614\n",
      "10/606 - Running Loss: 0.6315\n",
      "15/606 - Running Loss: 0.6700\n",
      "20/606 - Running Loss: 0.6006\n",
      "25/606 - Running Loss: 0.5842\n",
      "30/606 - Running Loss: 0.5650\n",
      "35/606 - Running Loss: 0.5532\n",
      "40/606 - Running Loss: 0.5367\n",
      "45/606 - Running Loss: 0.5265\n",
      "50/606 - Running Loss: 0.5733\n",
      "55/606 - Running Loss: 0.6007\n",
      "60/606 - Running Loss: 0.5900\n",
      "65/606 - Running Loss: 0.4813\n",
      "70/606 - Running Loss: 0.5374\n",
      "75/606 - Running Loss: 0.4359\n",
      "80/606 - Running Loss: 0.4277\n",
      "85/606 - Running Loss: 0.4105\n",
      "90/606 - Running Loss: 0.3830\n",
      "95/606 - Running Loss: 0.5282\n",
      "100/606 - Running Loss: 0.3403\n",
      "105/606 - Running Loss: 0.6498\n",
      "110/606 - Running Loss: 0.3085\n",
      "115/606 - Running Loss: 0.8157\n",
      "120/606 - Running Loss: 0.3130\n",
      "125/606 - Running Loss: 0.2698\n",
      "130/606 - Running Loss: 0.2718\n",
      "135/606 - Running Loss: 0.5640\n",
      "140/606 - Running Loss: 0.4860\n",
      "145/606 - Running Loss: 0.2049\n",
      "150/606 - Running Loss: 0.2095\n",
      "155/606 - Running Loss: 0.1712\n",
      "160/606 - Running Loss: 0.5311\n",
      "165/606 - Running Loss: 0.1751\n",
      "170/606 - Running Loss: 0.1547\n",
      "175/606 - Running Loss: 0.1674\n",
      "180/606 - Running Loss: 0.1730\n",
      "185/606 - Running Loss: 0.1664\n",
      "190/606 - Running Loss: 0.3583\n",
      "195/606 - Running Loss: 0.1338\n",
      "200/606 - Running Loss: 0.2247\n",
      "205/606 - Running Loss: 0.1204\n",
      "210/606 - Running Loss: 0.1631\n",
      "215/606 - Running Loss: 0.5703\n",
      "220/606 - Running Loss: 0.1289\n",
      "225/606 - Running Loss: 0.1404\n",
      "230/606 - Running Loss: 0.2044\n",
      "235/606 - Running Loss: 0.1358\n",
      "240/606 - Running Loss: 0.5980\n",
      "245/606 - Running Loss: 0.1728\n",
      "250/606 - Running Loss: 0.7596\n",
      "255/606 - Running Loss: 0.2122\n",
      "260/606 - Running Loss: 0.2159\n",
      "265/606 - Running Loss: 0.1880\n",
      "270/606 - Running Loss: 0.7221\n",
      "275/606 - Running Loss: 0.6053\n",
      "280/606 - Running Loss: 0.1784\n",
      "285/606 - Running Loss: 0.2192\n",
      "290/606 - Running Loss: 0.6124\n",
      "295/606 - Running Loss: 0.2069\n",
      "300/606 - Running Loss: 0.2273\n",
      "305/606 - Running Loss: 0.2423\n",
      "310/606 - Running Loss: 0.1851\n",
      "315/606 - Running Loss: 0.1915\n",
      "320/606 - Running Loss: 0.2070\n",
      "325/606 - Running Loss: 0.8195\n",
      "330/606 - Running Loss: 0.2075\n",
      "335/606 - Running Loss: 0.3610\n",
      "340/606 - Running Loss: 0.2406\n",
      "345/606 - Running Loss: 0.7390\n",
      "350/606 - Running Loss: 0.5532\n",
      "355/606 - Running Loss: 0.4679\n",
      "360/606 - Running Loss: 0.2201\n",
      "365/606 - Running Loss: 0.2185\n",
      "370/606 - Running Loss: 0.2005\n",
      "375/606 - Running Loss: 0.1516\n",
      "380/606 - Running Loss: 0.1543\n",
      "385/606 - Running Loss: 0.1843\n",
      "390/606 - Running Loss: 0.4346\n",
      "395/606 - Running Loss: 0.1710\n",
      "400/606 - Running Loss: 0.5605\n",
      "405/606 - Running Loss: 0.1178\n",
      "410/606 - Running Loss: 0.7578\n",
      "415/606 - Running Loss: 0.1425\n",
      "420/606 - Running Loss: 0.1544\n",
      "425/606 - Running Loss: 0.1855\n",
      "430/606 - Running Loss: 0.1849\n",
      "435/606 - Running Loss: 0.5070\n",
      "440/606 - Running Loss: 0.1895\n",
      "445/606 - Running Loss: 0.6627\n",
      "450/606 - Running Loss: 0.1879\n",
      "455/606 - Running Loss: 0.9023\n",
      "460/606 - Running Loss: 0.6119\n",
      "465/606 - Running Loss: 0.1807\n",
      "470/606 - Running Loss: 0.6738\n",
      "475/606 - Running Loss: 0.1710\n",
      "480/606 - Running Loss: 0.1797\n",
      "485/606 - Running Loss: 0.1243\n",
      "490/606 - Running Loss: 0.6296\n",
      "495/606 - Running Loss: 0.5193\n",
      "500/606 - Running Loss: 0.1519\n",
      "505/606 - Running Loss: 0.1872\n",
      "510/606 - Running Loss: 0.1996\n",
      "515/606 - Running Loss: 0.8466\n",
      "520/606 - Running Loss: 0.1867\n",
      "525/606 - Running Loss: 0.8147\n",
      "530/606 - Running Loss: 0.1390\n",
      "535/606 - Running Loss: 0.5978\n",
      "540/606 - Running Loss: 1.0688\n",
      "545/606 - Running Loss: 0.4210\n",
      "550/606 - Running Loss: 0.2021\n",
      "555/606 - Running Loss: 0.2177\n",
      "560/606 - Running Loss: 0.4362\n",
      "565/606 - Running Loss: 0.2055\n",
      "570/606 - Running Loss: 0.1714\n",
      "575/606 - Running Loss: 0.1758\n",
      "580/606 - Running Loss: 0.1962\n",
      "585/606 - Running Loss: 0.1733\n",
      "590/606 - Running Loss: 0.6240\n",
      "595/606 - Running Loss: 0.3431\n",
      "600/606 - Running Loss: 0.1322\n",
      "605/606 - Running Loss: 0.2440\n",
      "0/20 - Train Loss: 0.0004\n",
      "0/606 - Running Loss: 1.0958\n",
      "5/606 - Running Loss: 0.8465\n",
      "10/606 - Running Loss: 0.2844\n",
      "15/606 - Running Loss: 0.1905\n",
      "20/606 - Running Loss: 0.1920\n",
      "25/606 - Running Loss: 0.2186\n",
      "30/606 - Running Loss: 0.1848\n",
      "35/606 - Running Loss: 0.1727\n",
      "40/606 - Running Loss: 0.1622\n",
      "45/606 - Running Loss: 0.2310\n",
      "50/606 - Running Loss: 0.1409\n",
      "55/606 - Running Loss: 0.1269\n",
      "60/606 - Running Loss: 0.1111\n",
      "65/606 - Running Loss: 0.2655\n",
      "70/606 - Running Loss: 0.1245\n",
      "75/606 - Running Loss: 0.1428\n",
      "80/606 - Running Loss: 0.2060\n",
      "85/606 - Running Loss: 0.8794\n",
      "90/606 - Running Loss: 0.2068\n",
      "95/606 - Running Loss: 0.6725\n",
      "100/606 - Running Loss: 0.1667\n",
      "105/606 - Running Loss: 0.1530\n",
      "110/606 - Running Loss: 0.1698\n",
      "115/606 - Running Loss: 0.6464\n",
      "120/606 - Running Loss: 0.1990\n",
      "125/606 - Running Loss: 0.4616\n",
      "130/606 - Running Loss: 0.2225\n",
      "135/606 - Running Loss: 0.1962\n",
      "140/606 - Running Loss: 0.4386\n",
      "145/606 - Running Loss: 0.6814\n",
      "150/606 - Running Loss: 0.7702\n",
      "155/606 - Running Loss: 0.7291\n",
      "160/606 - Running Loss: 0.1649\n",
      "165/606 - Running Loss: 0.1529\n",
      "170/606 - Running Loss: 0.2058\n",
      "175/606 - Running Loss: 0.1572\n",
      "180/606 - Running Loss: 0.3969\n",
      "185/606 - Running Loss: 0.1961\n",
      "190/606 - Running Loss: 0.1467\n",
      "195/606 - Running Loss: 0.1519\n",
      "200/606 - Running Loss: 0.1056\n",
      "205/606 - Running Loss: 0.2064\n",
      "210/606 - Running Loss: 0.1782\n",
      "215/606 - Running Loss: 0.1512\n",
      "220/606 - Running Loss: 0.1118\n",
      "225/606 - Running Loss: 0.1163\n",
      "230/606 - Running Loss: 0.1374\n",
      "235/606 - Running Loss: 0.4827\n",
      "240/606 - Running Loss: 0.2825\n",
      "245/606 - Running Loss: 0.1573\n",
      "250/606 - Running Loss: 0.6248\n",
      "255/606 - Running Loss: 0.1682\n",
      "260/606 - Running Loss: 0.6240\n",
      "265/606 - Running Loss: 0.1758\n",
      "270/606 - Running Loss: 0.6451\n",
      "275/606 - Running Loss: 0.1841\n",
      "280/606 - Running Loss: 0.4944\n",
      "285/606 - Running Loss: 1.0005\n",
      "290/606 - Running Loss: 0.5849\n",
      "295/606 - Running Loss: 0.1859\n",
      "300/606 - Running Loss: 0.4621\n",
      "305/606 - Running Loss: 0.4174\n",
      "310/606 - Running Loss: 0.3744\n",
      "315/606 - Running Loss: 0.2040\n",
      "320/606 - Running Loss: 0.1990\n",
      "325/606 - Running Loss: 0.2046\n",
      "330/606 - Running Loss: 0.2171\n",
      "335/606 - Running Loss: 0.2517\n",
      "340/606 - Running Loss: 0.4275\n",
      "345/606 - Running Loss: 0.7874\n",
      "350/606 - Running Loss: 0.1934\n",
      "355/606 - Running Loss: 0.1897\n",
      "360/606 - Running Loss: 0.7129\n",
      "365/606 - Running Loss: 0.5198\n",
      "370/606 - Running Loss: 0.1783\n",
      "375/606 - Running Loss: 0.1900\n",
      "380/606 - Running Loss: 0.4235\n",
      "385/606 - Running Loss: 0.5801\n",
      "390/606 - Running Loss: 0.6757\n",
      "395/606 - Running Loss: 0.1907\n",
      "400/606 - Running Loss: 0.1789\n",
      "405/606 - Running Loss: 0.1883\n",
      "410/606 - Running Loss: 0.9577\n",
      "415/606 - Running Loss: 0.1800\n",
      "420/606 - Running Loss: 0.1691\n",
      "425/606 - Running Loss: 0.1637\n",
      "430/606 - Running Loss: 0.4479\n",
      "435/606 - Running Loss: 0.6729\n",
      "440/606 - Running Loss: 0.2845\n",
      "445/606 - Running Loss: 0.9019\n",
      "450/606 - Running Loss: 0.6703\n",
      "455/606 - Running Loss: 0.9548\n",
      "460/606 - Running Loss: 0.1724\n",
      "465/606 - Running Loss: 0.1404\n",
      "470/606 - Running Loss: 0.1323\n",
      "475/606 - Running Loss: 0.6714\n",
      "480/606 - Running Loss: 0.3517\n",
      "485/606 - Running Loss: 0.2770\n",
      "490/606 - Running Loss: 0.8252\n",
      "495/606 - Running Loss: 0.7901\n",
      "500/606 - Running Loss: 0.1706\n",
      "505/606 - Running Loss: 0.1968\n",
      "510/606 - Running Loss: 0.1970\n",
      "515/606 - Running Loss: 0.1782\n",
      "520/606 - Running Loss: 0.2379\n",
      "525/606 - Running Loss: 0.6231\n",
      "530/606 - Running Loss: 0.2640\n",
      "535/606 - Running Loss: 0.8652\n",
      "540/606 - Running Loss: 0.5802\n",
      "545/606 - Running Loss: 0.5643\n",
      "550/606 - Running Loss: 0.3363\n",
      "555/606 - Running Loss: 0.5882\n",
      "560/606 - Running Loss: 0.1584\n",
      "565/606 - Running Loss: 0.1675\n",
      "570/606 - Running Loss: 0.6097\n",
      "575/606 - Running Loss: 0.4507\n",
      "580/606 - Running Loss: 0.2383\n",
      "585/606 - Running Loss: 0.6664\n",
      "590/606 - Running Loss: 0.1605\n",
      "595/606 - Running Loss: 0.7557\n",
      "600/606 - Running Loss: 0.2184\n",
      "605/606 - Running Loss: 0.4030\n",
      "1/20 - Train Loss: 0.0007\n",
      "0/606 - Running Loss: 0.4552\n",
      "5/606 - Running Loss: 0.2933\n",
      "10/606 - Running Loss: 0.7715\n",
      "15/606 - Running Loss: 0.2615\n",
      "20/606 - Running Loss: 0.4821\n",
      "25/606 - Running Loss: 0.1770\n",
      "30/606 - Running Loss: 0.6403\n",
      "35/606 - Running Loss: 0.1889\n",
      "40/606 - Running Loss: 0.1613\n",
      "45/606 - Running Loss: 0.1696\n",
      "50/606 - Running Loss: 0.5035\n",
      "55/606 - Running Loss: 0.1585\n",
      "60/606 - Running Loss: 0.1776\n",
      "65/606 - Running Loss: 0.6597\n",
      "70/606 - Running Loss: 0.5331\n",
      "75/606 - Running Loss: 0.8721\n",
      "80/606 - Running Loss: 0.1381\n",
      "85/606 - Running Loss: 0.7035\n",
      "90/606 - Running Loss: 0.1370\n",
      "95/606 - Running Loss: 0.1520\n",
      "100/606 - Running Loss: 0.1623\n",
      "105/606 - Running Loss: 0.2893\n",
      "110/606 - Running Loss: 0.2145\n",
      "115/606 - Running Loss: 0.1621\n",
      "120/606 - Running Loss: 0.1528\n",
      "125/606 - Running Loss: 0.6037\n",
      "130/606 - Running Loss: 0.2819\n",
      "135/606 - Running Loss: 0.7249\n",
      "140/606 - Running Loss: 0.1660\n",
      "145/606 - Running Loss: 0.2385\n",
      "150/606 - Running Loss: 0.6370\n",
      "155/606 - Running Loss: 0.1953\n",
      "160/606 - Running Loss: 0.1979\n",
      "165/606 - Running Loss: 0.2302\n",
      "170/606 - Running Loss: 0.2076\n",
      "175/606 - Running Loss: 0.8354\n",
      "180/606 - Running Loss: 0.1762\n",
      "185/606 - Running Loss: 0.1702\n",
      "190/606 - Running Loss: 0.6072\n",
      "195/606 - Running Loss: 0.1719\n",
      "200/606 - Running Loss: 0.2356\n",
      "205/606 - Running Loss: 0.2020\n",
      "210/606 - Running Loss: 0.2857\n",
      "215/606 - Running Loss: 0.5181\n",
      "220/606 - Running Loss: 0.1797\n",
      "225/606 - Running Loss: 0.1620\n",
      "230/606 - Running Loss: 0.1556\n",
      "235/606 - Running Loss: 0.9260\n",
      "240/606 - Running Loss: 0.1322\n",
      "245/606 - Running Loss: 0.4277\n",
      "250/606 - Running Loss: 0.1889\n",
      "255/606 - Running Loss: 0.1647\n",
      "260/606 - Running Loss: 0.2117\n",
      "265/606 - Running Loss: 0.5254\n",
      "270/606 - Running Loss: 0.4733\n",
      "275/606 - Running Loss: 0.4499\n",
      "280/606 - Running Loss: 0.2501\n",
      "285/606 - Running Loss: 0.1689\n",
      "290/606 - Running Loss: 0.5449\n",
      "295/606 - Running Loss: 0.2115\n",
      "300/606 - Running Loss: 0.1968\n",
      "305/606 - Running Loss: 0.1783\n",
      "310/606 - Running Loss: 0.7089\n",
      "315/606 - Running Loss: 0.1444\n",
      "320/606 - Running Loss: 0.2218\n",
      "325/606 - Running Loss: 0.2076\n",
      "330/606 - Running Loss: 0.1946\n",
      "335/606 - Running Loss: 0.1790\n",
      "340/606 - Running Loss: 0.2056\n",
      "345/606 - Running Loss: 0.5504\n",
      "350/606 - Running Loss: 0.5779\n",
      "355/606 - Running Loss: 0.5343\n",
      "360/606 - Running Loss: 0.4493\n",
      "365/606 - Running Loss: 0.2434\n",
      "370/606 - Running Loss: 0.1522\n",
      "375/606 - Running Loss: 0.6685\n",
      "380/606 - Running Loss: 0.1950\n",
      "385/606 - Running Loss: 0.1768\n",
      "390/606 - Running Loss: 0.1630\n",
      "395/606 - Running Loss: 0.5017\n",
      "400/606 - Running Loss: 0.1969\n",
      "405/606 - Running Loss: 0.6285\n",
      "410/606 - Running Loss: 0.9886\n",
      "415/606 - Running Loss: 0.1754\n",
      "420/606 - Running Loss: 0.4361\n",
      "425/606 - Running Loss: 0.1893\n",
      "430/606 - Running Loss: 0.4230\n",
      "435/606 - Running Loss: 0.9489\n",
      "440/606 - Running Loss: 0.2012\n",
      "445/606 - Running Loss: 0.5017\n",
      "450/606 - Running Loss: 1.0161\n",
      "455/606 - Running Loss: 0.1909\n",
      "460/606 - Running Loss: 0.2223\n",
      "465/606 - Running Loss: 0.9158\n",
      "470/606 - Running Loss: 0.8232\n",
      "475/606 - Running Loss: 0.1574\n",
      "480/606 - Running Loss: 0.1869\n",
      "485/606 - Running Loss: 0.5626\n",
      "490/606 - Running Loss: 0.1693\n",
      "495/606 - Running Loss: 0.1850\n",
      "500/606 - Running Loss: 1.2788\n",
      "505/606 - Running Loss: 0.1701\n",
      "510/606 - Running Loss: 0.1885\n",
      "515/606 - Running Loss: 0.5167\n",
      "520/606 - Running Loss: 0.1862\n",
      "525/606 - Running Loss: 0.1866\n",
      "530/606 - Running Loss: 0.2122\n",
      "535/606 - Running Loss: 0.6402\n",
      "540/606 - Running Loss: 0.7289\n",
      "545/606 - Running Loss: 0.1614\n",
      "550/606 - Running Loss: 0.1744\n",
      "555/606 - Running Loss: 1.0155\n",
      "560/606 - Running Loss: 0.1692\n",
      "565/606 - Running Loss: 0.4080\n",
      "570/606 - Running Loss: 0.1841\n",
      "575/606 - Running Loss: 0.1463\n",
      "580/606 - Running Loss: 1.4095\n",
      "585/606 - Running Loss: 0.1604\n",
      "590/606 - Running Loss: 0.1556\n",
      "595/606 - Running Loss: 0.1890\n",
      "600/606 - Running Loss: 0.2762\n",
      "605/606 - Running Loss: 0.6502\n",
      "2/20 - Train Loss: 0.0011\n",
      "0/606 - Running Loss: 0.1391\n",
      "5/606 - Running Loss: 0.7648\n",
      "10/606 - Running Loss: 1.0038\n",
      "15/606 - Running Loss: 0.3010\n",
      "20/606 - Running Loss: 0.1445\n",
      "25/606 - Running Loss: 0.5380\n",
      "30/606 - Running Loss: 0.1455\n",
      "35/606 - Running Loss: 0.1378\n",
      "40/606 - Running Loss: 0.1694\n",
      "45/606 - Running Loss: 1.0223\n",
      "50/606 - Running Loss: 0.6744\n",
      "55/606 - Running Loss: 0.6073\n",
      "60/606 - Running Loss: 0.2206\n",
      "65/606 - Running Loss: 0.8530\n",
      "70/606 - Running Loss: 0.2195\n",
      "75/606 - Running Loss: 0.2060\n",
      "80/606 - Running Loss: 0.5860\n",
      "85/606 - Running Loss: 0.6614\n",
      "90/606 - Running Loss: 0.8558\n",
      "95/606 - Running Loss: 0.2331\n",
      "100/606 - Running Loss: 0.5220\n",
      "105/606 - Running Loss: 0.4079\n",
      "110/606 - Running Loss: 0.1567\n",
      "115/606 - Running Loss: 0.1216\n",
      "120/606 - Running Loss: 0.6171\n",
      "125/606 - Running Loss: 0.4997\n",
      "130/606 - Running Loss: 0.1799\n",
      "135/606 - Running Loss: 0.5419\n",
      "140/606 - Running Loss: 0.4813\n",
      "145/606 - Running Loss: 0.1806\n",
      "150/606 - Running Loss: 0.1858\n",
      "155/606 - Running Loss: 0.2143\n",
      "160/606 - Running Loss: 0.2032\n",
      "165/606 - Running Loss: 0.5978\n",
      "170/606 - Running Loss: 0.6389\n",
      "175/606 - Running Loss: 0.4491\n",
      "180/606 - Running Loss: 0.2126\n",
      "185/606 - Running Loss: 0.3557\n",
      "190/606 - Running Loss: 1.2812\n",
      "195/606 - Running Loss: 0.2162\n",
      "200/606 - Running Loss: 0.1792\n",
      "205/606 - Running Loss: 0.1774\n",
      "210/606 - Running Loss: 0.6670\n",
      "215/606 - Running Loss: 0.2579\n",
      "220/606 - Running Loss: 0.1548\n",
      "225/606 - Running Loss: 0.1327\n",
      "230/606 - Running Loss: 0.1568\n",
      "235/606 - Running Loss: 0.5550\n",
      "240/606 - Running Loss: 0.4224\n",
      "245/606 - Running Loss: 0.2018\n",
      "250/606 - Running Loss: 0.1489\n",
      "255/606 - Running Loss: 0.1446\n",
      "260/606 - Running Loss: 0.3560\n",
      "265/606 - Running Loss: 0.1749\n",
      "270/606 - Running Loss: 0.8774\n",
      "275/606 - Running Loss: 0.2107\n",
      "280/606 - Running Loss: 0.1963\n",
      "285/606 - Running Loss: 0.3241\n",
      "290/606 - Running Loss: 0.6563\n",
      "295/606 - Running Loss: 0.2079\n",
      "300/606 - Running Loss: 0.2121\n",
      "305/606 - Running Loss: 0.1595\n",
      "310/606 - Running Loss: 0.3913\n",
      "315/606 - Running Loss: 0.1746\n",
      "320/606 - Running Loss: 0.6544\n",
      "325/606 - Running Loss: 0.1562\n",
      "330/606 - Running Loss: 0.1497\n",
      "335/606 - Running Loss: 0.1533\n",
      "340/606 - Running Loss: 0.1945\n",
      "345/606 - Running Loss: 0.7715\n",
      "350/606 - Running Loss: 0.1912\n",
      "355/606 - Running Loss: 0.6382\n",
      "360/606 - Running Loss: 0.2217\n",
      "365/606 - Running Loss: 0.1846\n",
      "370/606 - Running Loss: 0.4127\n",
      "375/606 - Running Loss: 0.1862\n",
      "380/606 - Running Loss: 0.1595\n",
      "385/606 - Running Loss: 0.1750\n",
      "390/606 - Running Loss: 0.5668\n",
      "395/606 - Running Loss: 0.6489\n",
      "400/606 - Running Loss: 0.1799\n",
      "405/606 - Running Loss: 0.1159\n",
      "410/606 - Running Loss: 0.6737\n",
      "415/606 - Running Loss: 0.1607\n",
      "420/606 - Running Loss: 0.1420\n",
      "425/606 - Running Loss: 0.5317\n",
      "430/606 - Running Loss: 0.4147\n",
      "435/606 - Running Loss: 0.6357\n",
      "440/606 - Running Loss: 0.1378\n",
      "445/606 - Running Loss: 0.1824\n",
      "450/606 - Running Loss: 0.4283\n",
      "455/606 - Running Loss: 0.4556\n",
      "460/606 - Running Loss: 0.1550\n",
      "465/606 - Running Loss: 0.5162\n",
      "470/606 - Running Loss: 0.7643\n",
      "475/606 - Running Loss: 0.5173\n",
      "480/606 - Running Loss: 0.8096\n",
      "485/606 - Running Loss: 0.5825\n",
      "490/606 - Running Loss: 0.2035\n",
      "495/606 - Running Loss: 0.6346\n",
      "500/606 - Running Loss: 0.2343\n",
      "505/606 - Running Loss: 0.2787\n",
      "510/606 - Running Loss: 0.7160\n",
      "515/606 - Running Loss: 0.2109\n",
      "520/606 - Running Loss: 0.2569\n",
      "525/606 - Running Loss: 0.2025\n",
      "530/606 - Running Loss: 0.5206\n",
      "535/606 - Running Loss: 0.4912\n",
      "540/606 - Running Loss: 0.6046\n",
      "545/606 - Running Loss: 0.1695\n",
      "550/606 - Running Loss: 0.5366\n",
      "555/606 - Running Loss: 0.2483\n",
      "560/606 - Running Loss: 0.4936\n",
      "565/606 - Running Loss: 0.4286\n",
      "570/606 - Running Loss: 0.5883\n",
      "575/606 - Running Loss: 0.1453\n",
      "580/606 - Running Loss: 0.1452\n",
      "585/606 - Running Loss: 0.5227\n",
      "590/606 - Running Loss: 0.1599\n",
      "595/606 - Running Loss: 0.1764\n",
      "600/606 - Running Loss: 0.9475\n",
      "605/606 - Running Loss: 0.3430\n",
      "3/20 - Train Loss: 0.0006\n",
      "0/606 - Running Loss: 0.1287\n",
      "5/606 - Running Loss: 0.1813\n",
      "10/606 - Running Loss: 0.4287\n",
      "15/606 - Running Loss: 0.7683\n",
      "20/606 - Running Loss: 0.1855\n",
      "25/606 - Running Loss: 1.3375\n",
      "30/606 - Running Loss: 0.4375\n",
      "35/606 - Running Loss: 0.5822\n",
      "40/606 - Running Loss: 0.5040\n",
      "45/606 - Running Loss: 0.5200\n",
      "50/606 - Running Loss: 0.2101\n",
      "55/606 - Running Loss: 0.1896\n",
      "60/606 - Running Loss: 0.5230\n",
      "65/606 - Running Loss: 0.2125\n",
      "70/606 - Running Loss: 0.1785\n",
      "75/606 - Running Loss: 0.3566\n",
      "80/606 - Running Loss: 0.4757\n",
      "85/606 - Running Loss: 0.2159\n",
      "90/606 - Running Loss: 0.4101\n",
      "95/606 - Running Loss: 0.1867\n",
      "100/606 - Running Loss: 0.5063\n",
      "105/606 - Running Loss: 0.1879\n",
      "110/606 - Running Loss: 0.1491\n",
      "115/606 - Running Loss: 0.5335\n",
      "120/606 - Running Loss: 0.1479\n",
      "125/606 - Running Loss: 0.7373\n",
      "130/606 - Running Loss: 0.1573\n",
      "135/606 - Running Loss: 0.1327\n",
      "140/606 - Running Loss: 1.0607\n",
      "145/606 - Running Loss: 0.4595\n",
      "150/606 - Running Loss: 0.9874\n",
      "155/606 - Running Loss: 0.2031\n",
      "160/606 - Running Loss: 0.5571\n",
      "165/606 - Running Loss: 0.2085\n",
      "170/606 - Running Loss: 0.1699\n",
      "175/606 - Running Loss: 0.2228\n",
      "180/606 - Running Loss: 0.2246\n",
      "185/606 - Running Loss: 0.1531\n",
      "190/606 - Running Loss: 0.2042\n",
      "195/606 - Running Loss: 0.6705\n",
      "200/606 - Running Loss: 0.1814\n",
      "205/606 - Running Loss: 0.2293\n",
      "210/606 - Running Loss: 0.2896\n",
      "215/606 - Running Loss: 0.1755\n",
      "220/606 - Running Loss: 0.1982\n",
      "225/606 - Running Loss: 0.8302\n",
      "230/606 - Running Loss: 0.5442\n",
      "235/606 - Running Loss: 0.6784\n",
      "240/606 - Running Loss: 0.8546\n",
      "245/606 - Running Loss: 0.1941\n",
      "250/606 - Running Loss: 0.4209\n",
      "255/606 - Running Loss: 0.1744\n",
      "260/606 - Running Loss: 0.1851\n",
      "265/606 - Running Loss: 0.1782\n",
      "270/606 - Running Loss: 0.8774\n",
      "275/606 - Running Loss: 0.1438\n",
      "280/606 - Running Loss: 0.1310\n",
      "285/606 - Running Loss: 0.5735\n",
      "290/606 - Running Loss: 0.1730\n",
      "295/606 - Running Loss: 0.1482\n",
      "300/606 - Running Loss: 0.2276\n",
      "305/606 - Running Loss: 0.6399\n",
      "310/606 - Running Loss: 0.1119\n",
      "315/606 - Running Loss: 0.8206\n",
      "320/606 - Running Loss: 0.4898\n",
      "325/606 - Running Loss: 0.2338\n",
      "330/606 - Running Loss: 0.6384\n",
      "335/606 - Running Loss: 0.4635\n",
      "340/606 - Running Loss: 0.3157\n",
      "345/606 - Running Loss: 0.1627\n",
      "350/606 - Running Loss: 0.2443\n",
      "355/606 - Running Loss: 0.7411\n",
      "360/606 - Running Loss: 0.3507\n",
      "365/606 - Running Loss: 0.4067\n",
      "370/606 - Running Loss: 0.1428\n",
      "375/606 - Running Loss: 0.1726\n",
      "380/606 - Running Loss: 0.6328\n",
      "385/606 - Running Loss: 0.6128\n",
      "390/606 - Running Loss: 0.5288\n",
      "395/606 - Running Loss: 0.3178\n",
      "400/606 - Running Loss: 1.4768\n",
      "405/606 - Running Loss: 0.2034\n",
      "410/606 - Running Loss: 0.5642\n",
      "415/606 - Running Loss: 0.1819\n",
      "420/606 - Running Loss: 0.6804\n",
      "425/606 - Running Loss: 0.4763\n",
      "430/606 - Running Loss: 0.1888\n",
      "435/606 - Running Loss: 0.6306\n",
      "440/606 - Running Loss: 0.2105\n",
      "445/606 - Running Loss: 0.8066\n",
      "450/606 - Running Loss: 0.1840\n",
      "455/606 - Running Loss: 0.2658\n",
      "460/606 - Running Loss: 0.8123\n",
      "465/606 - Running Loss: 0.1912\n",
      "470/606 - Running Loss: 0.2588\n",
      "475/606 - Running Loss: 0.6599\n",
      "480/606 - Running Loss: 0.4764\n",
      "485/606 - Running Loss: 0.6694\n",
      "490/606 - Running Loss: 0.7221\n",
      "495/606 - Running Loss: 0.1978\n",
      "500/606 - Running Loss: 0.6434\n",
      "505/606 - Running Loss: 0.1761\n",
      "510/606 - Running Loss: 0.4927\n",
      "515/606 - Running Loss: 0.1730\n",
      "520/606 - Running Loss: 0.1768\n",
      "525/606 - Running Loss: 0.5980\n",
      "530/606 - Running Loss: 0.5378\n",
      "535/606 - Running Loss: 0.1671\n",
      "540/606 - Running Loss: 0.1792\n",
      "545/606 - Running Loss: 0.1494\n",
      "550/606 - Running Loss: 0.6418\n",
      "555/606 - Running Loss: 0.4578\n",
      "560/606 - Running Loss: 0.3559\n",
      "565/606 - Running Loss: 0.1445\n",
      "570/606 - Running Loss: 0.9913\n",
      "575/606 - Running Loss: 0.2376\n",
      "580/606 - Running Loss: 0.5403\n",
      "585/606 - Running Loss: 0.1819\n",
      "590/606 - Running Loss: 0.1946\n",
      "595/606 - Running Loss: 0.1781\n",
      "600/606 - Running Loss: 0.4515\n",
      "605/606 - Running Loss: 0.1279\n",
      "4/20 - Train Loss: 0.0002\n",
      "0/606 - Running Loss: 0.8204\n",
      "5/606 - Running Loss: 0.1380\n",
      "10/606 - Running Loss: 0.3945\n",
      "15/606 - Running Loss: 0.1593\n",
      "20/606 - Running Loss: 0.8448\n",
      "25/606 - Running Loss: 0.1550\n",
      "30/606 - Running Loss: 0.4642\n",
      "35/606 - Running Loss: 0.2300\n",
      "40/606 - Running Loss: 0.4790\n",
      "45/606 - Running Loss: 0.2030\n",
      "50/606 - Running Loss: 0.6566\n",
      "55/606 - Running Loss: 0.2460\n",
      "60/606 - Running Loss: 0.1854\n",
      "65/606 - Running Loss: 0.1855\n",
      "70/606 - Running Loss: 0.4911\n",
      "75/606 - Running Loss: 0.1846\n",
      "80/606 - Running Loss: 0.1545\n",
      "85/606 - Running Loss: 0.1836\n",
      "90/606 - Running Loss: 0.2256\n",
      "95/606 - Running Loss: 0.9424\n",
      "100/606 - Running Loss: 0.5347\n",
      "105/606 - Running Loss: 0.1532\n",
      "110/606 - Running Loss: 0.1931\n",
      "115/606 - Running Loss: 0.4542\n",
      "120/606 - Running Loss: 0.1955\n",
      "125/606 - Running Loss: 0.1820\n",
      "130/606 - Running Loss: 0.5295\n",
      "135/606 - Running Loss: 0.1821\n",
      "140/606 - Running Loss: 0.5036\n",
      "145/606 - Running Loss: 0.5800\n",
      "150/606 - Running Loss: 0.1922\n",
      "155/606 - Running Loss: 0.5977\n",
      "160/606 - Running Loss: 0.3034\n",
      "165/606 - Running Loss: 0.1484\n",
      "170/606 - Running Loss: 1.1205\n",
      "175/606 - Running Loss: 0.3475\n",
      "180/606 - Running Loss: 0.1597\n",
      "185/606 - Running Loss: 0.1556\n",
      "190/606 - Running Loss: 0.1095\n",
      "195/606 - Running Loss: 0.2397\n",
      "200/606 - Running Loss: 0.5504\n",
      "205/606 - Running Loss: 0.1779\n",
      "210/606 - Running Loss: 0.1428\n",
      "215/606 - Running Loss: 0.7654\n",
      "220/606 - Running Loss: 0.8163\n",
      "225/606 - Running Loss: 0.2379\n",
      "230/606 - Running Loss: 0.4421\n",
      "235/606 - Running Loss: 0.3194\n",
      "240/606 - Running Loss: 0.1870\n",
      "245/606 - Running Loss: 0.4823\n",
      "250/606 - Running Loss: 0.1452\n",
      "255/606 - Running Loss: 0.4860\n",
      "260/606 - Running Loss: 0.1592\n",
      "265/606 - Running Loss: 0.6264\n",
      "270/606 - Running Loss: 0.1341\n",
      "275/606 - Running Loss: 0.1438\n",
      "280/606 - Running Loss: 0.3019\n",
      "285/606 - Running Loss: 0.2378\n",
      "290/606 - Running Loss: 0.1615\n",
      "295/606 - Running Loss: 0.6964\n",
      "300/606 - Running Loss: 0.5810\n",
      "305/606 - Running Loss: 0.1844\n",
      "310/606 - Running Loss: 0.1875\n",
      "315/606 - Running Loss: 0.1842\n",
      "320/606 - Running Loss: 0.6266\n",
      "325/606 - Running Loss: 0.6638\n",
      "330/606 - Running Loss: 0.5881\n",
      "335/606 - Running Loss: 0.2452\n",
      "340/606 - Running Loss: 0.7796\n",
      "345/606 - Running Loss: 0.1671\n",
      "350/606 - Running Loss: 0.2122\n",
      "355/606 - Running Loss: 0.4191\n",
      "360/606 - Running Loss: 0.7765\n",
      "365/606 - Running Loss: 0.4246\n",
      "370/606 - Running Loss: 0.1867\n",
      "375/606 - Running Loss: 0.2065\n",
      "380/606 - Running Loss: 0.5820\n",
      "385/606 - Running Loss: 0.4175\n",
      "390/606 - Running Loss: 0.4866\n",
      "395/606 - Running Loss: 0.1860\n",
      "400/606 - Running Loss: 0.2159\n",
      "405/606 - Running Loss: 0.5752\n",
      "410/606 - Running Loss: 0.5964\n",
      "415/606 - Running Loss: 0.1844\n",
      "420/606 - Running Loss: 0.5113\n",
      "425/606 - Running Loss: 0.4285\n",
      "430/606 - Running Loss: 0.1700\n",
      "435/606 - Running Loss: 0.1990\n",
      "440/606 - Running Loss: 0.2086\n",
      "445/606 - Running Loss: 0.6792\n",
      "450/606 - Running Loss: 0.5629\n",
      "455/606 - Running Loss: 0.1899\n",
      "460/606 - Running Loss: 0.1353\n",
      "465/606 - Running Loss: 0.1962\n",
      "470/606 - Running Loss: 0.5614\n",
      "475/606 - Running Loss: 0.4883\n",
      "480/606 - Running Loss: 0.1758\n",
      "485/606 - Running Loss: 0.1468\n",
      "490/606 - Running Loss: 0.1220\n",
      "495/606 - Running Loss: 1.1777\n",
      "500/606 - Running Loss: 0.1591\n",
      "505/606 - Running Loss: 0.2518\n",
      "510/606 - Running Loss: 0.1427\n",
      "515/606 - Running Loss: 0.9103\n",
      "520/606 - Running Loss: 0.1857\n",
      "525/606 - Running Loss: 0.5123\n",
      "530/606 - Running Loss: 0.8152\n",
      "535/606 - Running Loss: 0.2362\n",
      "540/606 - Running Loss: 0.1633\n",
      "545/606 - Running Loss: 0.1654\n",
      "550/606 - Running Loss: 0.3437\n",
      "555/606 - Running Loss: 0.1557\n",
      "560/606 - Running Loss: 0.1624\n",
      "565/606 - Running Loss: 0.3139\n",
      "570/606 - Running Loss: 0.1837\n",
      "575/606 - Running Loss: 0.1809\n",
      "580/606 - Running Loss: 0.4207\n",
      "585/606 - Running Loss: 0.4033\n",
      "590/606 - Running Loss: 0.1367\n",
      "595/606 - Running Loss: 0.4046\n",
      "600/606 - Running Loss: 0.1505\n",
      "605/606 - Running Loss: 0.1800\n",
      "5/20 - Train Loss: 0.0003\n",
      "0/606 - Running Loss: 0.8133\n",
      "5/606 - Running Loss: 0.9718\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:9\u001b[0m\n",
      "Cell \u001b[0;32mIn[17], line 27\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(trainloader, model, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 27\u001b[0m running_loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m idx \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = EPOCHS\n",
    "\n",
    "best_epoch = 0\n",
    "best_score = 0.0\n",
    "train_loss = []\n",
    "# val_loss, val_dice_coefficient = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = train_one_epoch(trainloader, model, optimizer, criterion, DEVICE)\n",
    "    train_loss.append(losses[\"train\"])\n",
    "    # val_loss.append(losses[\"val\"])\n",
    "    \n",
    "    print(f\"{epoch}/{num_epochs} - Train Loss: {losses['train']:.4f}\")\n",
    "    \n",
    "    if epoch % 2 ==0:\n",
    "        save_model(model.state_dict(), f\"model_{epoch:02d}.pth\")\n",
    "        \n",
    "print(f\"Best epoch: {best_epoch} -> Best Dice Coeffient: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88837590-f157-4479-9ef0-08ec7e27a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. weight 저장후, inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee27ead-84fa-4902-a91a-095189fd055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9. 평가지표로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807919ee-3355-4cf2-a1f9-cf8e98ed2bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f9a62-2684-4cb7-963e-1e49a4674a25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
