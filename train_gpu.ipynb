{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73dea87-08a4-4601-94c5-bd71c7f94b5e",
   "metadata": {},
   "source": [
    "### 1. 모델 구현\n",
    "### 2. 데이터 클래스\n",
    "### 3. 데이터 로더\n",
    "### 4. 손실함수\n",
    "### 5. optimizer\n",
    "### 6. weight 저장후, inference\n",
    "### 7. 평가지표로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd85c93d-7928-4515-babd-711c88a6fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. 모델 구현 OK\n",
    "# 직사각형 input, 직사각형 output 되도록 kernel사이즈나 그런것들 조절해주기\n",
    "### 2. 데이터 클래스 @\n",
    "#  run length encoded된거 decode하기 @\n",
    "### 3. 데이터 로더 @\n",
    "### 4. Transformer, collate_fn 구현 -----> Proceeding...\n",
    "# 데이터 클래스 이전에 구현해서 데이터 인스턴스에 넣어주기\n",
    "### 5. 손실함수\n",
    "### 6. optimizer\n",
    "### 7. weight 저장후, inference\n",
    "### 8. 평가지표로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bfcc53-0d4c-486a-b41c-c151fb673221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa45a44-3048-4108-afc3-c3b7e2472c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.모델구현\n",
    "class Unet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.ConPathLayer1 = nn.Sequential(\n",
    "        # torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(1,3), stride=1, padding=(25,0), padding_mode='reflect', dilation=(1,67)),\n",
    "        torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.ConPathLayer2 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.ConPathLayer3 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.ConPathLayer4 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool4 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.BottomLayer = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv1 = torch.nn.ConvTranspose2d(in_channels=1024, out_channels=512,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer1 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv2 = torch.nn.ConvTranspose2d(in_channels=512, out_channels=256,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer2 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv3 = torch.nn.ConvTranspose2d(in_channels=256, out_channels=128,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer3 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv4 = torch.nn.ConvTranspose2d(in_channels=128, out_channels=64,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer4 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1, stride=1)\n",
    "        # torch.nn.Conv2d(in_channels=64, out_channels=2, kernel_size=(1,1), stride=1, padding=(66,158), padding_mode='replicate', dilation=(1,1))\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.ConPathLayer1(x)\n",
    "    skip1 = torchvision.transforms.CenterCrop(392)(x)\n",
    "    x = self.MaxPool1(x)\n",
    "    x = self.ConPathLayer2(x)\n",
    "    skip2 = torchvision.transforms.CenterCrop(200)(x)\n",
    "    x = self.MaxPool2(x)\n",
    "    x = self.ConPathLayer3(x)\n",
    "    skip3 = torchvision.transforms.CenterCrop(104)(x)\n",
    "    x = self.MaxPool3(x)\n",
    "    x = self.ConPathLayer4(x)\n",
    "    skip4 = torchvision.transforms.CenterCrop(56)(x)\n",
    "    x = self.MaxPool4(x)\n",
    "    # print(\"skip1:{}, skip2:{}, skip3:{}, skip4:{}\".format(skip1.shape, skip2.shape, skip3.shape, skip4.shape))\n",
    "    x = self.BottomLayer(x)\n",
    "    x = self.UpConv1(x)\n",
    "    x = torch.cat((skip4, x), dim=1)\n",
    "    x = self.ExpPathLayer1(x)\n",
    "    x = self.UpConv2(x)\n",
    "    x = torch.cat((skip3, x), dim=1)\n",
    "    x = self.ExpPathLayer2(x)\n",
    "    x = self.UpConv3(x)\n",
    "    x = torch.cat((skip2, x), dim=1)\n",
    "    x = self.ExpPathLayer3(x)\n",
    "    x = self.UpConv4(x)\n",
    "    x = torch.cat((skip1, x), dim=1)\n",
    "    result = self.ExpPathLayer4(x)\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74bd23fd-79da-4485-be34-25fb697da161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 데이터 클래스 정의 \n",
    "class CT_Dataset():\n",
    "    def __init__(self, phase, transformer=None):\n",
    "        # self.train_csv_path = \"C:/Users/syb62/Desktop/Unet/train/train.csv\"\n",
    "        # self.train_path = \"C:/Users/syb62/Desktop/Unet/train\"\n",
    "        self.train_csv_path = \"/home/inbic/Desktop/Unet/train/train.csv\"\n",
    "        self.train_path = \"/home/inbic/Desktop/Unet/train\"\n",
    "        \n",
    "        self.phase = phase\n",
    "        self.train_img_list = [] # train 이미지들 목록\n",
    "        self.image_info = collections.defaultdict(dict) # train이미지 id, 이미지 경로, 라벨(annotation)\n",
    "        self.transformer = transformer\n",
    "        \n",
    "        ################ train이미지 목록 리스트 생성 ################\n",
    "        train_list = os.listdir(self.train_path)\n",
    "        for i in train_list:\n",
    "          if i.endswith(\".png\"):\n",
    "            self.train_img_list.append(i)\n",
    "\n",
    "        ################ image_info 생성 ################\n",
    "        train_df = pd.read_csv(self.train_csv_path)\n",
    "        anno_list=[]\n",
    "        df_ids = np.array(train_df.loc[:, \"id\"].values).tolist()\n",
    "        mask_dic = {}\n",
    "\n",
    "        for img_name in self.train_img_list:\n",
    "          id = img_name.split(\".\")[0]\n",
    "          anno_list=[]\n",
    "          for j in range(len(df_ids)):\n",
    "            if df_ids[j] == id:\n",
    "              a = train_df.loc[:, \"annotation\"].values[j]\n",
    "              anno_list.append(a)\n",
    "            else:\n",
    "              pass\n",
    "          mask_dic[id]=anno_list\n",
    "\n",
    "          for i in range(len(mask_dic.keys())):\n",
    "            mask_dic_keys = list(mask_dic.keys())\n",
    "            self.image_info[i]={\n",
    "                    'image_id': mask_dic_keys[i],\n",
    "                    'image_path': os.path.join(self.train_path, mask_dic_keys[i] + '.png'),\n",
    "                    'annotations': mask_dic[mask_dic_keys[i]]\n",
    "                    }\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.train_img_list)\n",
    "    \n",
    "    def rle_decode(self, mask_rle, shape, color=1):\n",
    "        '''\n",
    "        mask_rle: run-length as string formated (start length)\n",
    "        shape: (height,width) of array to return \n",
    "        Returns numpy array, 1 - mask, 0 - background\n",
    "        '''\n",
    "        s = mask_rle.split()\n",
    "        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "        starts -= 1\n",
    "        ends = starts + lengths\n",
    "        img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n",
    "        for lo, hi in zip(starts, ends):\n",
    "            img[lo : hi] = color\n",
    "        return img.reshape(shape)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        info = self.image_info[idx]\n",
    "        img_path = info[\"image_path\"]\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        ##########################\n",
    "\n",
    "        HEIGHT = 520\n",
    "        WIDTH = 704\n",
    "        # HEIGHT = 388\n",
    "        # WIDTH = 388\n",
    "        height = HEIGHT\n",
    "        width = WIDTH\n",
    "\n",
    "        masks = np.zeros((len(info['annotations']), height, width), dtype=np.uint8)\n",
    "        labels=[]\n",
    "\n",
    "        for i, annotation in enumerate(info['annotations']):\n",
    "            a_mask = self.rle_decode(annotation, (HEIGHT, WIDTH))\n",
    "            a_mask = Image.fromarray(a_mask)\n",
    "\n",
    "            # if self.should_resize:\n",
    "            if True:\n",
    "                a_mask = a_mask.resize((width, height), resample=Image.BILINEAR)\n",
    "                a_mask = np.array(a_mask) > 0\n",
    "                labels.append(a_mask)\n",
    "                masks[i, :, :] = a_mask\n",
    "        ##########################\n",
    "        total = np.zeros((520, 704))\n",
    "        # total = np.zeros((388, 388))\n",
    "        for i in range(len(labels)):\n",
    "            total = total + labels[i]\n",
    "        \n",
    "        total= (total>=1)\n",
    "        # total = total.reshape(((1, 520, 704)))\n",
    "        target = total\n",
    "        \n",
    "        if self.transformer:\n",
    "            image = self.transformer(image)\n",
    "        \n",
    "        target = torch.from_numpy(target).long()\n",
    "        \n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445ca2b8-3112-4bfd-b571-ff66e49ecd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Transformer & collate_fn 구현\n",
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Resize(size=(572,572)),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb69e02f-1b2f-48b2-813c-7c947e02c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for a, b in batch: \n",
    "        images.append(a)\n",
    "        targets.append(b)\n",
    "    images = torch.stack(images, dim=0) \n",
    "    targets = torch.stack(targets, dim=0)\n",
    "\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b8a05f1-5f87-4b19-af36-a582855f8dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. 손실함수 계산 Class 선언\n",
    "class Unet_loss():\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes=num_classes\n",
    "        self.CE_loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        # [B, 2, 520, 704] 크기의 prediction입력\n",
    "        # self.prediction=prediction\n",
    "        # [B, 520, 704] 크기의 target입력\n",
    "        # self.target=target\n",
    "    def __call__(self, prediction, target):\n",
    "        loss = self.get_loss(prediction, target)\n",
    "        return loss\n",
    "    \n",
    "    def get_loss(self,  prediction, target):\n",
    "        ################ Dice Coefficient 적용할거면 연산 진행해서 넣어주기 ################\n",
    "        # prediction: [B, 2, 520, 704] -> [B, 2, 520, 704] 예측한 label값으로 변환\n",
    "        predictions_ = torch.argmax(prediction, dim=1)\n",
    "        onehot_pred = F.one_hot(predictions_,num_classes=self.num_classes)\n",
    "        onehot_pred = onehot_pred.permute(0,3,1,2)\n",
    "        \n",
    "        # prediction: [B, 520, 704] -> [B, 2, 520, 704] 을 예측한 label값으로 변환\n",
    "        onehot_target = F.one_hot(target , num_classes=self.num_classes).permute(0,3,1,2)\n",
    "        ########################################################################################\n",
    "        \n",
    "        # CE loss 계산\n",
    "        # ce_loss = self.CE_loss(onehot_pred, onehot_target)\n",
    "        ce_loss = self.CE_loss(prediction, target)\n",
    "        \n",
    "        return ce_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c2d6f91-9f3a-41c1-aa82-8d46d06be89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Training\n",
    "def train_one_epoch(trainloader, model,optimizer,criterion,device):\n",
    "    losses = {}\n",
    "    # for phase in ['train', 'val']:\n",
    "    for phase in ['train']:\n",
    "        running_loss = 0.0\n",
    "\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        for idx, batch in enumerate(trainloader):\n",
    "            train_batch_data = batch[0].to(device)\n",
    "            target_batch = batch[1].to(device)\n",
    "            \n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                pred = model(train_batch_data)\n",
    "                loss = criterion(pred,target_batch)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                optimizer.zero_grad() \n",
    "                # optimizer의 갱신할 기울기 값을, 즉, 연결되어있는 모든 model params의 기울기값들을 초기화\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss = loss.item()\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                if idx % 5 == 0:\n",
    "                    text = f\"{idx}/{len(trainloader)}\" + \\\n",
    "                            f\" - Running Loss: {loss.item():.4f}\"\n",
    "                    print(text)\n",
    "            \n",
    "        losses[phase] = running_loss / len(trainloader)\n",
    "                    \n",
    "    # print(f\"idx:{idx}, batch image: {train_batch_data.shape}, \\\n",
    "    # batch target:{target_batch.shape}, output: {pred.shape}, loss: {loss}\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9db5a28-930f-4bd6-aa3e-e22c4d56af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_state, model_name, save_dir=\"/home/inbic/Desktop/Unet/models\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model_state, os.path.join(save_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ba5e42a-75bd-45cc-b78f-8263d9d7eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = True\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available and is_cuda else 'cpu')\n",
    "# DEVICE = torch.device('cpu')\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 20\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "train_data = CT_Dataset(phase='train', transformer=transformer)\n",
    "trainloader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn,shuffle=True)\n",
    "model = Unet()\n",
    "model = model.to(DEVICE)\n",
    "criterion = Unet_loss(num_classes=NUM_CLASSES)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= LEARNING_RATE, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff4c7a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68e5303b-3f0b-40c6-9b34-dbc57ae601e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/606 - Running Loss: 0.6448\n",
      "5/606 - Running Loss: 0.6377\n",
      "10/606 - Running Loss: 0.6259\n",
      "15/606 - Running Loss: 0.6620\n",
      "20/606 - Running Loss: 0.5939\n",
      "25/606 - Running Loss: 0.5800\n",
      "30/606 - Running Loss: 0.6078\n",
      "35/606 - Running Loss: 0.5544\n",
      "40/606 - Running Loss: 0.5319\n",
      "45/606 - Running Loss: 0.5157\n",
      "50/606 - Running Loss: 0.5211\n",
      "55/606 - Running Loss: 0.4873\n",
      "60/606 - Running Loss: 0.5830\n",
      "65/606 - Running Loss: 0.4886\n",
      "70/606 - Running Loss: 0.5324\n",
      "75/606 - Running Loss: 0.6654\n",
      "80/606 - Running Loss: 0.4151\n",
      "85/606 - Running Loss: 0.4357\n",
      "90/606 - Running Loss: 0.3927\n",
      "95/606 - Running Loss: 0.3814\n",
      "100/606 - Running Loss: 0.3939\n",
      "105/606 - Running Loss: 0.3760\n",
      "110/606 - Running Loss: 0.4450\n",
      "115/606 - Running Loss: 0.5277\n",
      "120/606 - Running Loss: 0.5227\n",
      "125/606 - Running Loss: 0.5164\n",
      "130/606 - Running Loss: 0.2782\n",
      "135/606 - Running Loss: 0.2837\n",
      "140/606 - Running Loss: 0.2588\n",
      "145/606 - Running Loss: 0.2418\n",
      "150/606 - Running Loss: 0.2226\n",
      "155/606 - Running Loss: 0.5182\n",
      "160/606 - Running Loss: 0.1923\n",
      "165/606 - Running Loss: 0.5150\n",
      "170/606 - Running Loss: 0.7005\n",
      "175/606 - Running Loss: 0.4329\n",
      "180/606 - Running Loss: 0.7796\n",
      "185/606 - Running Loss: 0.4519\n",
      "190/606 - Running Loss: 0.2010\n",
      "195/606 - Running Loss: 0.1847\n",
      "200/606 - Running Loss: 0.5217\n",
      "205/606 - Running Loss: 0.4143\n",
      "210/606 - Running Loss: 0.3801\n",
      "215/606 - Running Loss: 0.4632\n",
      "220/606 - Running Loss: 0.1319\n",
      "225/606 - Running Loss: 0.1337\n",
      "230/606 - Running Loss: 0.1604\n",
      "235/606 - Running Loss: 0.6221\n",
      "240/606 - Running Loss: 0.5950\n",
      "245/606 - Running Loss: 0.1444\n",
      "250/606 - Running Loss: 0.1343\n",
      "255/606 - Running Loss: 0.6664\n",
      "260/606 - Running Loss: 0.1192\n",
      "265/606 - Running Loss: 0.6564\n",
      "270/606 - Running Loss: 0.7046\n",
      "275/606 - Running Loss: 0.1745\n",
      "280/606 - Running Loss: 0.1375\n",
      "285/606 - Running Loss: 0.5234\n",
      "290/606 - Running Loss: 0.1660\n",
      "295/606 - Running Loss: 0.1563\n",
      "300/606 - Running Loss: 0.1561\n",
      "305/606 - Running Loss: 0.1117\n",
      "310/606 - Running Loss: 0.1172\n",
      "315/606 - Running Loss: 0.1476\n",
      "320/606 - Running Loss: 0.1437\n",
      "325/606 - Running Loss: 0.4639\n",
      "330/606 - Running Loss: 0.1645\n",
      "335/606 - Running Loss: 0.1504\n",
      "340/606 - Running Loss: 1.4443\n",
      "345/606 - Running Loss: 0.5874\n",
      "350/606 - Running Loss: 0.1680\n",
      "355/606 - Running Loss: 0.6577\n",
      "360/606 - Running Loss: 0.1742\n",
      "365/606 - Running Loss: 0.4721\n",
      "370/606 - Running Loss: 0.8143\n",
      "375/606 - Running Loss: 0.1690\n",
      "380/606 - Running Loss: 0.1793\n",
      "385/606 - Running Loss: 0.1584\n",
      "390/606 - Running Loss: 0.1550\n",
      "395/606 - Running Loss: 0.6261\n",
      "400/606 - Running Loss: 0.5705\n",
      "405/606 - Running Loss: 0.5357\n",
      "410/606 - Running Loss: 0.2113\n",
      "415/606 - Running Loss: 0.5831\n",
      "420/606 - Running Loss: 0.1513\n",
      "425/606 - Running Loss: 0.1286\n",
      "430/606 - Running Loss: 0.4597\n",
      "435/606 - Running Loss: 0.1808\n",
      "440/606 - Running Loss: 0.8342\n",
      "445/606 - Running Loss: 0.1269\n",
      "450/606 - Running Loss: 0.1502\n",
      "455/606 - Running Loss: 0.1347\n",
      "460/606 - Running Loss: 0.1389\n",
      "465/606 - Running Loss: 0.5567\n",
      "470/606 - Running Loss: 0.7921\n",
      "475/606 - Running Loss: 0.1792\n",
      "480/606 - Running Loss: 0.1842\n",
      "485/606 - Running Loss: 0.4769\n",
      "490/606 - Running Loss: 0.7125\n",
      "495/606 - Running Loss: 0.2605\n",
      "500/606 - Running Loss: 0.1887\n",
      "505/606 - Running Loss: 0.5459\n",
      "510/606 - Running Loss: 0.1824\n",
      "515/606 - Running Loss: 0.1683\n",
      "520/606 - Running Loss: 0.3874\n",
      "525/606 - Running Loss: 0.5281\n",
      "530/606 - Running Loss: 0.5471\n",
      "535/606 - Running Loss: 0.6074\n",
      "540/606 - Running Loss: 0.1637\n",
      "545/606 - Running Loss: 0.1291\n",
      "550/606 - Running Loss: 0.7205\n",
      "555/606 - Running Loss: 0.4973\n",
      "560/606 - Running Loss: 0.1524\n",
      "565/606 - Running Loss: 0.6655\n",
      "570/606 - Running Loss: 0.1344\n",
      "575/606 - Running Loss: 0.1687\n",
      "580/606 - Running Loss: 0.4481\n",
      "585/606 - Running Loss: 0.1719\n",
      "590/606 - Running Loss: 0.6225\n",
      "595/606 - Running Loss: 0.5401\n",
      "600/606 - Running Loss: 0.2409\n",
      "605/606 - Running Loss: 0.1827\n",
      "0/20 - Train Loss: 0.0003\n",
      "0/606 - Running Loss: 0.1366\n",
      "5/606 - Running Loss: 0.2655\n",
      "10/606 - Running Loss: 0.1458\n",
      "15/606 - Running Loss: 0.1290\n",
      "20/606 - Running Loss: 0.3300\n",
      "25/606 - Running Loss: 0.1473\n",
      "30/606 - Running Loss: 0.5025\n",
      "35/606 - Running Loss: 0.1134\n",
      "40/606 - Running Loss: 0.1224\n",
      "45/606 - Running Loss: 0.1534\n",
      "50/606 - Running Loss: 0.1678\n",
      "55/606 - Running Loss: 1.5008\n",
      "60/606 - Running Loss: 0.1321\n",
      "65/606 - Running Loss: 0.1569\n",
      "70/606 - Running Loss: 0.5069\n",
      "75/606 - Running Loss: 0.6595\n",
      "80/606 - Running Loss: 0.1648\n",
      "85/606 - Running Loss: 0.1803\n",
      "90/606 - Running Loss: 0.2145\n",
      "95/606 - Running Loss: 0.5917\n",
      "100/606 - Running Loss: 0.4402\n",
      "105/606 - Running Loss: 0.4544\n",
      "110/606 - Running Loss: 0.7228\n",
      "115/606 - Running Loss: 0.1757\n",
      "120/606 - Running Loss: 0.4865\n",
      "125/606 - Running Loss: 0.1698\n",
      "130/606 - Running Loss: 0.1497\n",
      "135/606 - Running Loss: 0.1326\n",
      "140/606 - Running Loss: 0.1537\n",
      "145/606 - Running Loss: 0.2210\n",
      "150/606 - Running Loss: 0.6349\n",
      "155/606 - Running Loss: 0.7307\n",
      "160/606 - Running Loss: 0.6365\n",
      "165/606 - Running Loss: 0.1239\n",
      "170/606 - Running Loss: 0.4282\n",
      "175/606 - Running Loss: 0.1509\n",
      "180/606 - Running Loss: 0.5577\n",
      "185/606 - Running Loss: 0.4485\n",
      "190/606 - Running Loss: 0.1737\n",
      "195/606 - Running Loss: 0.4993\n",
      "200/606 - Running Loss: 0.4261\n",
      "205/606 - Running Loss: 0.1605\n",
      "210/606 - Running Loss: 0.1436\n",
      "215/606 - Running Loss: 0.1436\n",
      "220/606 - Running Loss: 0.1402\n",
      "225/606 - Running Loss: 0.1517\n",
      "230/606 - Running Loss: 0.1616\n",
      "235/606 - Running Loss: 0.1610\n",
      "240/606 - Running Loss: 0.2024\n",
      "245/606 - Running Loss: 0.6099\n",
      "250/606 - Running Loss: 0.4660\n",
      "255/606 - Running Loss: 0.4399\n",
      "260/606 - Running Loss: 0.1394\n",
      "265/606 - Running Loss: 0.1521\n",
      "270/606 - Running Loss: 0.8127\n",
      "275/606 - Running Loss: 0.1247\n",
      "280/606 - Running Loss: 0.1486\n",
      "285/606 - Running Loss: 0.1280\n",
      "290/606 - Running Loss: 0.3944\n",
      "295/606 - Running Loss: 0.1694\n",
      "300/606 - Running Loss: 0.4804\n",
      "305/606 - Running Loss: 0.2178\n",
      "310/606 - Running Loss: 0.1759\n",
      "315/606 - Running Loss: 0.2108\n",
      "320/606 - Running Loss: 0.2281\n",
      "325/606 - Running Loss: 0.1959\n",
      "330/606 - Running Loss: 0.2302\n",
      "335/606 - Running Loss: 0.5007\n",
      "340/606 - Running Loss: 0.4801\n",
      "345/606 - Running Loss: 0.1660\n",
      "350/606 - Running Loss: 0.1751\n",
      "355/606 - Running Loss: 0.1491\n",
      "360/606 - Running Loss: 0.1902\n",
      "365/606 - Running Loss: 0.3763\n",
      "370/606 - Running Loss: 0.5371\n",
      "375/606 - Running Loss: 0.1516\n",
      "380/606 - Running Loss: 0.2104\n",
      "385/606 - Running Loss: 0.1404\n",
      "390/606 - Running Loss: 0.5970\n",
      "395/606 - Running Loss: 0.4136\n",
      "400/606 - Running Loss: 0.1803\n",
      "405/606 - Running Loss: 0.1515\n",
      "410/606 - Running Loss: 0.1705\n",
      "415/606 - Running Loss: 0.5512\n",
      "420/606 - Running Loss: 0.1478\n",
      "425/606 - Running Loss: 0.2675\n",
      "430/606 - Running Loss: 0.1508\n",
      "435/606 - Running Loss: 1.0306\n",
      "440/606 - Running Loss: 0.7544\n",
      "445/606 - Running Loss: 0.4507\n",
      "450/606 - Running Loss: 0.1835\n",
      "455/606 - Running Loss: 0.3749\n",
      "460/606 - Running Loss: 0.7966\n",
      "465/606 - Running Loss: 0.1817\n",
      "470/606 - Running Loss: 0.3843\n",
      "475/606 - Running Loss: 0.4343\n",
      "480/606 - Running Loss: 0.1922\n",
      "485/606 - Running Loss: 0.1264\n",
      "490/606 - Running Loss: 0.1523\n",
      "495/606 - Running Loss: 0.1433\n",
      "500/606 - Running Loss: 0.1400\n",
      "505/606 - Running Loss: 0.1812\n",
      "510/606 - Running Loss: 0.2750\n",
      "515/606 - Running Loss: 0.1079\n",
      "520/606 - Running Loss: 0.4300\n",
      "525/606 - Running Loss: 0.1859\n",
      "530/606 - Running Loss: 0.5500\n",
      "535/606 - Running Loss: 0.1535\n",
      "540/606 - Running Loss: 0.7349\n",
      "545/606 - Running Loss: 0.1496\n",
      "550/606 - Running Loss: 0.3972\n",
      "555/606 - Running Loss: 0.3798\n",
      "560/606 - Running Loss: 0.6490\n",
      "565/606 - Running Loss: 0.5951\n",
      "570/606 - Running Loss: 0.1535\n",
      "575/606 - Running Loss: 0.1898\n",
      "580/606 - Running Loss: 0.3936\n",
      "585/606 - Running Loss: 0.1886\n",
      "590/606 - Running Loss: 0.6080\n",
      "595/606 - Running Loss: 0.1681\n",
      "600/606 - Running Loss: 0.1722\n",
      "605/606 - Running Loss: 0.1531\n",
      "1/20 - Train Loss: 0.0003\n",
      "0/606 - Running Loss: 0.5130\n",
      "5/606 - Running Loss: 0.4469\n",
      "10/606 - Running Loss: 0.1634\n",
      "15/606 - Running Loss: 0.1630\n",
      "20/606 - Running Loss: 0.2992\n",
      "25/606 - Running Loss: 0.1377\n",
      "30/606 - Running Loss: 0.2843\n",
      "35/606 - Running Loss: 0.1930\n",
      "40/606 - Running Loss: 0.4710\n",
      "45/606 - Running Loss: 0.1194\n",
      "50/606 - Running Loss: 0.6937\n",
      "55/606 - Running Loss: 0.1070\n",
      "60/606 - Running Loss: 1.4846\n",
      "65/606 - Running Loss: 0.1383\n",
      "70/606 - Running Loss: 0.1398\n",
      "75/606 - Running Loss: 0.1568\n",
      "80/606 - Running Loss: 0.6266\n",
      "85/606 - Running Loss: 0.7027\n",
      "90/606 - Running Loss: 0.5660\n",
      "95/606 - Running Loss: 0.4952\n",
      "100/606 - Running Loss: 0.1554\n",
      "105/606 - Running Loss: 0.1776\n",
      "110/606 - Running Loss: 0.5488\n",
      "115/606 - Running Loss: 0.1268\n",
      "120/606 - Running Loss: 0.1675\n",
      "125/606 - Running Loss: 0.1453\n",
      "130/606 - Running Loss: 0.2909\n",
      "135/606 - Running Loss: 0.7797\n",
      "140/606 - Running Loss: 0.1381\n",
      "145/606 - Running Loss: 0.1485\n",
      "150/606 - Running Loss: 0.1280\n",
      "155/606 - Running Loss: 0.1160\n",
      "160/606 - Running Loss: 0.6097\n",
      "165/606 - Running Loss: 1.4925\n",
      "170/606 - Running Loss: 0.1732\n",
      "175/606 - Running Loss: 0.6070\n",
      "180/606 - Running Loss: 0.1836\n",
      "185/606 - Running Loss: 0.1405\n",
      "190/606 - Running Loss: 0.1598\n",
      "195/606 - Running Loss: 0.1759\n",
      "200/606 - Running Loss: 0.1937\n",
      "205/606 - Running Loss: 0.1907\n",
      "210/606 - Running Loss: 0.6540\n",
      "215/606 - Running Loss: 0.3868\n",
      "220/606 - Running Loss: 0.1532\n",
      "225/606 - Running Loss: 0.1510\n",
      "230/606 - Running Loss: 0.1533\n",
      "235/606 - Running Loss: 0.1482\n",
      "240/606 - Running Loss: 0.1607\n",
      "245/606 - Running Loss: 0.7028\n",
      "250/606 - Running Loss: 0.1174\n",
      "255/606 - Running Loss: 0.8263\n",
      "260/606 - Running Loss: 0.6114\n",
      "265/606 - Running Loss: 0.1268\n",
      "270/606 - Running Loss: 0.1507\n",
      "275/606 - Running Loss: 0.4328\n",
      "280/606 - Running Loss: 0.1344\n",
      "285/606 - Running Loss: 0.5704\n",
      "290/606 - Running Loss: 0.1481\n",
      "295/606 - Running Loss: 0.1715\n",
      "300/606 - Running Loss: 0.4953\n",
      "305/606 - Running Loss: 0.1771\n",
      "310/606 - Running Loss: 0.5213\n",
      "315/606 - Running Loss: 0.1522\n",
      "320/606 - Running Loss: 0.4512\n",
      "325/606 - Running Loss: 0.6747\n",
      "330/606 - Running Loss: 0.1531\n",
      "335/606 - Running Loss: 0.4940\n",
      "340/606 - Running Loss: 0.1574\n",
      "345/606 - Running Loss: 0.2008\n",
      "350/606 - Running Loss: 0.5373\n",
      "355/606 - Running Loss: 0.1744\n",
      "360/606 - Running Loss: 0.1397\n",
      "365/606 - Running Loss: 0.7130\n",
      "370/606 - Running Loss: 0.9213\n",
      "375/606 - Running Loss: 0.1445\n",
      "380/606 - Running Loss: 0.7207\n",
      "385/606 - Running Loss: 0.1534\n",
      "390/606 - Running Loss: 0.1910\n",
      "395/606 - Running Loss: 0.1744\n",
      "400/606 - Running Loss: 0.2493\n",
      "405/606 - Running Loss: 0.6986\n",
      "410/606 - Running Loss: 0.5406\n",
      "415/606 - Running Loss: 0.2434\n",
      "420/606 - Running Loss: 0.4782\n",
      "425/606 - Running Loss: 0.2528\n",
      "430/606 - Running Loss: 0.1444\n",
      "435/606 - Running Loss: 0.6502\n",
      "440/606 - Running Loss: 0.7037\n",
      "445/606 - Running Loss: 0.1546\n",
      "450/606 - Running Loss: 0.8011\n",
      "455/606 - Running Loss: 0.1456\n",
      "460/606 - Running Loss: 0.3013\n",
      "465/606 - Running Loss: 0.5267\n",
      "470/606 - Running Loss: 0.4595\n",
      "475/606 - Running Loss: 0.1527\n",
      "480/606 - Running Loss: 0.6668\n",
      "485/606 - Running Loss: 0.5857\n",
      "490/606 - Running Loss: 0.1449\n",
      "495/606 - Running Loss: 0.1617\n",
      "500/606 - Running Loss: 0.5497\n",
      "505/606 - Running Loss: 0.1314\n",
      "510/606 - Running Loss: 0.1925\n",
      "515/606 - Running Loss: 1.5024\n",
      "520/606 - Running Loss: 0.5750\n",
      "525/606 - Running Loss: 0.1799\n",
      "530/606 - Running Loss: 0.6864\n",
      "535/606 - Running Loss: 0.8076\n",
      "540/606 - Running Loss: 0.3843\n",
      "545/606 - Running Loss: 0.1704\n",
      "550/606 - Running Loss: 0.1362\n",
      "555/606 - Running Loss: 0.1891\n",
      "560/606 - Running Loss: 0.1395\n",
      "565/606 - Running Loss: 0.4317\n",
      "570/606 - Running Loss: 0.5813\n",
      "575/606 - Running Loss: 0.1577\n",
      "580/606 - Running Loss: 0.1274\n",
      "585/606 - Running Loss: 0.1928\n",
      "590/606 - Running Loss: 0.1552\n",
      "595/606 - Running Loss: 0.1432\n",
      "600/606 - Running Loss: 0.3214\n",
      "605/606 - Running Loss: 0.1467\n",
      "2/20 - Train Loss: 0.0002\n",
      "0/606 - Running Loss: 0.1935\n",
      "5/606 - Running Loss: 0.1884\n",
      "10/606 - Running Loss: 0.4539\n",
      "15/606 - Running Loss: 0.3541\n",
      "20/606 - Running Loss: 0.1383\n",
      "25/606 - Running Loss: 0.1518\n",
      "30/606 - Running Loss: 0.6019\n",
      "35/606 - Running Loss: 0.1582\n",
      "40/606 - Running Loss: 0.4615\n",
      "45/606 - Running Loss: 0.6756\n",
      "50/606 - Running Loss: 0.3914\n",
      "55/606 - Running Loss: 0.2118\n",
      "60/606 - Running Loss: 0.3169\n",
      "65/606 - Running Loss: 0.1740\n",
      "70/606 - Running Loss: 0.5682\n",
      "75/606 - Running Loss: 0.1337\n",
      "80/606 - Running Loss: 0.4663\n",
      "85/606 - Running Loss: 0.8481\n",
      "90/606 - Running Loss: 0.1217\n",
      "95/606 - Running Loss: 0.1382\n",
      "100/606 - Running Loss: 0.4154\n",
      "105/606 - Running Loss: 0.8034\n",
      "110/606 - Running Loss: 0.2141\n",
      "115/606 - Running Loss: 0.1883\n",
      "120/606 - Running Loss: 0.1732\n",
      "125/606 - Running Loss: 0.1411\n",
      "130/606 - Running Loss: 0.4277\n",
      "135/606 - Running Loss: 0.4608\n",
      "140/606 - Running Loss: 0.7870\n",
      "145/606 - Running Loss: 0.1942\n",
      "150/606 - Running Loss: 0.1818\n",
      "155/606 - Running Loss: 0.1665\n",
      "160/606 - Running Loss: 0.1382\n",
      "165/606 - Running Loss: 0.1676\n",
      "170/606 - Running Loss: 0.1410\n",
      "175/606 - Running Loss: 0.1620\n",
      "180/606 - Running Loss: 0.7898\n",
      "185/606 - Running Loss: 0.1376\n",
      "190/606 - Running Loss: 0.1243\n",
      "195/606 - Running Loss: 0.1937\n",
      "200/606 - Running Loss: 0.1585\n",
      "205/606 - Running Loss: 0.4558\n",
      "210/606 - Running Loss: 0.1383\n",
      "215/606 - Running Loss: 0.1538\n",
      "220/606 - Running Loss: 0.4359\n",
      "225/606 - Running Loss: 0.1384\n",
      "230/606 - Running Loss: 0.4895\n",
      "235/606 - Running Loss: 0.3879\n",
      "240/606 - Running Loss: 0.2019\n",
      "245/606 - Running Loss: 0.7572\n",
      "250/606 - Running Loss: 0.2185\n",
      "255/606 - Running Loss: 0.6088\n",
      "260/606 - Running Loss: 0.1533\n",
      "265/606 - Running Loss: 0.8085\n",
      "270/606 - Running Loss: 0.1269\n",
      "275/606 - Running Loss: 0.1195\n",
      "280/606 - Running Loss: 0.1365\n",
      "285/606 - Running Loss: 0.7435\n",
      "290/606 - Running Loss: 0.5843\n",
      "295/606 - Running Loss: 0.1165\n",
      "300/606 - Running Loss: 0.3807\n",
      "305/606 - Running Loss: 0.5432\n",
      "310/606 - Running Loss: 0.6227\n",
      "315/606 - Running Loss: 0.1380\n",
      "320/606 - Running Loss: 0.1684\n",
      "325/606 - Running Loss: 0.2305\n",
      "330/606 - Running Loss: 0.1538\n",
      "335/606 - Running Loss: 0.1922\n",
      "340/606 - Running Loss: 0.1281\n",
      "345/606 - Running Loss: 0.1513\n",
      "350/606 - Running Loss: 0.1519\n",
      "355/606 - Running Loss: 0.1180\n",
      "360/606 - Running Loss: 0.1422\n",
      "365/606 - Running Loss: 0.6747\n",
      "370/606 - Running Loss: 0.4306\n",
      "375/606 - Running Loss: 0.4540\n",
      "380/606 - Running Loss: 0.4469\n",
      "385/606 - Running Loss: 0.7116\n",
      "390/606 - Running Loss: 0.1391\n",
      "395/606 - Running Loss: 0.1674\n",
      "400/606 - Running Loss: 0.1694\n",
      "405/606 - Running Loss: 0.1477\n",
      "410/606 - Running Loss: 0.2551\n",
      "415/606 - Running Loss: 0.1589\n",
      "420/606 - Running Loss: 0.3262\n",
      "425/606 - Running Loss: 0.1573\n",
      "430/606 - Running Loss: 0.5254\n",
      "435/606 - Running Loss: 0.6053\n",
      "440/606 - Running Loss: 0.1418\n",
      "445/606 - Running Loss: 0.1356\n",
      "450/606 - Running Loss: 0.1746\n",
      "455/606 - Running Loss: 0.1248\n",
      "460/606 - Running Loss: 0.1304\n",
      "465/606 - Running Loss: 0.1024\n",
      "470/606 - Running Loss: 0.1715\n",
      "475/606 - Running Loss: 0.2200\n",
      "480/606 - Running Loss: 0.5418\n",
      "485/606 - Running Loss: 0.4880\n",
      "490/606 - Running Loss: 0.9429\n",
      "495/606 - Running Loss: 0.1211\n",
      "500/606 - Running Loss: 0.8649\n",
      "505/606 - Running Loss: 0.4447\n",
      "510/606 - Running Loss: 0.1973\n",
      "515/606 - Running Loss: 0.1537\n",
      "520/606 - Running Loss: 0.2533\n",
      "525/606 - Running Loss: 0.7103\n",
      "530/606 - Running Loss: 0.1879\n",
      "535/606 - Running Loss: 0.4795\n",
      "540/606 - Running Loss: 0.1905\n",
      "545/606 - Running Loss: 0.1515\n",
      "550/606 - Running Loss: 0.9406\n",
      "555/606 - Running Loss: 0.1689\n",
      "560/606 - Running Loss: 0.7660\n",
      "565/606 - Running Loss: 0.1632\n",
      "570/606 - Running Loss: 0.4485\n",
      "575/606 - Running Loss: 0.1953\n",
      "580/606 - Running Loss: 0.2647\n",
      "585/606 - Running Loss: 0.2539\n",
      "590/606 - Running Loss: 0.9687\n",
      "595/606 - Running Loss: 0.4724\n",
      "600/606 - Running Loss: 0.1857\n",
      "605/606 - Running Loss: 0.1724\n",
      "3/20 - Train Loss: 0.0003\n",
      "0/606 - Running Loss: 0.1533\n",
      "5/606 - Running Loss: 0.1423\n",
      "10/606 - Running Loss: 0.1393\n",
      "15/606 - Running Loss: 0.2132\n",
      "20/606 - Running Loss: 0.1451\n",
      "25/606 - Running Loss: 0.1464\n",
      "30/606 - Running Loss: 0.2146\n",
      "35/606 - Running Loss: 0.1451\n",
      "40/606 - Running Loss: 0.1794\n",
      "45/606 - Running Loss: 0.7410\n",
      "50/606 - Running Loss: 0.5742\n",
      "55/606 - Running Loss: 0.3521\n",
      "60/606 - Running Loss: 0.5593\n",
      "65/606 - Running Loss: 0.1231\n",
      "70/606 - Running Loss: 0.1217\n",
      "75/606 - Running Loss: 0.1408\n",
      "80/606 - Running Loss: 0.5452\n",
      "85/606 - Running Loss: 0.7888\n",
      "90/606 - Running Loss: 0.1754\n",
      "95/606 - Running Loss: 0.2488\n",
      "100/606 - Running Loss: 0.1179\n",
      "105/606 - Running Loss: 0.8635\n",
      "110/606 - Running Loss: 0.1745\n",
      "115/606 - Running Loss: 0.8314\n",
      "120/606 - Running Loss: 0.1458\n",
      "125/606 - Running Loss: 0.1561\n",
      "130/606 - Running Loss: 0.1798\n",
      "135/606 - Running Loss: 0.1531\n",
      "140/606 - Running Loss: 0.1837\n",
      "145/606 - Running Loss: 0.5892\n",
      "150/606 - Running Loss: 0.2868\n",
      "155/606 - Running Loss: 0.1742\n",
      "160/606 - Running Loss: 0.1522\n",
      "165/606 - Running Loss: 0.1666\n",
      "170/606 - Running Loss: 0.1468\n",
      "175/606 - Running Loss: 0.2435\n",
      "180/606 - Running Loss: 0.1672\n",
      "185/606 - Running Loss: 0.2029\n",
      "190/606 - Running Loss: 0.4468\n",
      "195/606 - Running Loss: 0.1462\n",
      "200/606 - Running Loss: 0.3098\n",
      "205/606 - Running Loss: 0.1352\n",
      "210/606 - Running Loss: 0.6032\n",
      "215/606 - Running Loss: 0.1837\n",
      "220/606 - Running Loss: 0.1335\n",
      "225/606 - Running Loss: 0.1464\n",
      "230/606 - Running Loss: 0.4162\n",
      "235/606 - Running Loss: 0.1112\n",
      "240/606 - Running Loss: 0.1430\n",
      "245/606 - Running Loss: 0.1257\n",
      "250/606 - Running Loss: 0.2146\n",
      "255/606 - Running Loss: 0.8015\n",
      "260/606 - Running Loss: 0.6157\n",
      "265/606 - Running Loss: 0.6289\n",
      "270/606 - Running Loss: 0.1304\n",
      "275/606 - Running Loss: 0.1571\n",
      "280/606 - Running Loss: 1.3381\n",
      "285/606 - Running Loss: 0.1416\n",
      "290/606 - Running Loss: 0.1731\n",
      "295/606 - Running Loss: 0.1988\n",
      "300/606 - Running Loss: 0.8107\n",
      "305/606 - Running Loss: 0.7009\n",
      "310/606 - Running Loss: 0.4962\n",
      "315/606 - Running Loss: 0.1556\n",
      "320/606 - Running Loss: 0.3858\n",
      "325/606 - Running Loss: 0.1815\n",
      "330/606 - Running Loss: 0.2931\n",
      "335/606 - Running Loss: 0.1277\n",
      "340/606 - Running Loss: 1.0088\n",
      "345/606 - Running Loss: 0.3430\n",
      "350/606 - Running Loss: 0.1510\n",
      "355/606 - Running Loss: 0.4275\n",
      "360/606 - Running Loss: 0.6706\n",
      "365/606 - Running Loss: 0.4921\n",
      "370/606 - Running Loss: 0.8377\n",
      "375/606 - Running Loss: 0.5908\n",
      "380/606 - Running Loss: 0.1757\n",
      "385/606 - Running Loss: 0.1798\n",
      "390/606 - Running Loss: 0.1269\n",
      "395/606 - Running Loss: 0.6027\n",
      "400/606 - Running Loss: 0.1602\n",
      "405/606 - Running Loss: 0.1327\n",
      "410/606 - Running Loss: 0.5086\n",
      "415/606 - Running Loss: 0.1344\n",
      "420/606 - Running Loss: 0.3831\n",
      "425/606 - Running Loss: 0.6305\n",
      "430/606 - Running Loss: 0.7332\n",
      "435/606 - Running Loss: 0.4894\n",
      "440/606 - Running Loss: 0.4363\n",
      "445/606 - Running Loss: 0.1397\n",
      "450/606 - Running Loss: 0.6550\n",
      "455/606 - Running Loss: 0.1601\n",
      "460/606 - Running Loss: 0.5292\n",
      "465/606 - Running Loss: 0.1400\n",
      "470/606 - Running Loss: 0.2124\n",
      "475/606 - Running Loss: 0.1500\n",
      "480/606 - Running Loss: 0.3799\n",
      "485/606 - Running Loss: 0.1563\n",
      "490/606 - Running Loss: 0.1688\n",
      "495/606 - Running Loss: 0.1396\n",
      "500/606 - Running Loss: 0.6301\n",
      "505/606 - Running Loss: 0.7263\n",
      "510/606 - Running Loss: 0.1132\n",
      "515/606 - Running Loss: 0.7293\n",
      "520/606 - Running Loss: 0.1367\n",
      "525/606 - Running Loss: 0.1649\n",
      "530/606 - Running Loss: 0.5014\n",
      "535/606 - Running Loss: 0.8820\n",
      "540/606 - Running Loss: 0.5821\n",
      "545/606 - Running Loss: 0.3446\n",
      "550/606 - Running Loss: 0.2808\n",
      "555/606 - Running Loss: 0.1615\n",
      "560/606 - Running Loss: 0.5158\n",
      "565/606 - Running Loss: 0.5791\n",
      "570/606 - Running Loss: 0.1615\n",
      "575/606 - Running Loss: 0.5256\n",
      "580/606 - Running Loss: 0.4183\n",
      "585/606 - Running Loss: 0.1574\n",
      "590/606 - Running Loss: 0.4564\n",
      "595/606 - Running Loss: 0.1678\n",
      "600/606 - Running Loss: 0.1700\n",
      "605/606 - Running Loss: 0.1498\n",
      "4/20 - Train Loss: 0.0002\n",
      "0/606 - Running Loss: 0.1378\n",
      "5/606 - Running Loss: 0.1640\n",
      "10/606 - Running Loss: 0.9549\n",
      "15/606 - Running Loss: 0.5244\n",
      "20/606 - Running Loss: 0.1481\n",
      "25/606 - Running Loss: 0.1388\n",
      "30/606 - Running Loss: 0.5475\n",
      "35/606 - Running Loss: 0.2905\n",
      "40/606 - Running Loss: 0.4992\n",
      "45/606 - Running Loss: 0.2957\n",
      "50/606 - Running Loss: 0.1632\n",
      "55/606 - Running Loss: 0.1332\n",
      "60/606 - Running Loss: 0.1438\n",
      "65/606 - Running Loss: 0.6908\n",
      "70/606 - Running Loss: 0.3696\n",
      "75/606 - Running Loss: 0.4539\n",
      "80/606 - Running Loss: 0.3857\n",
      "85/606 - Running Loss: 0.4578\n",
      "90/606 - Running Loss: 0.8015\n",
      "95/606 - Running Loss: 0.1362\n",
      "100/606 - Running Loss: 0.8709\n",
      "105/606 - Running Loss: 0.1345\n",
      "110/606 - Running Loss: 0.3860\n",
      "115/606 - Running Loss: 0.1366\n",
      "120/606 - Running Loss: 0.1013\n",
      "125/606 - Running Loss: 0.1467\n",
      "130/606 - Running Loss: 0.1564\n",
      "135/606 - Running Loss: 0.3975\n",
      "140/606 - Running Loss: 0.7219\n",
      "145/606 - Running Loss: 0.4722\n",
      "150/606 - Running Loss: 0.5966\n",
      "155/606 - Running Loss: 0.4661\n",
      "160/606 - Running Loss: 0.1536\n",
      "165/606 - Running Loss: 0.1213\n",
      "170/606 - Running Loss: 0.1540\n",
      "175/606 - Running Loss: 0.1288\n",
      "180/606 - Running Loss: 0.1363\n",
      "185/606 - Running Loss: 0.2371\n",
      "190/606 - Running Loss: 0.1013\n",
      "195/606 - Running Loss: 0.1307\n",
      "200/606 - Running Loss: 0.5996\n",
      "205/606 - Running Loss: 0.5698\n",
      "210/606 - Running Loss: 0.4806\n",
      "215/606 - Running Loss: 0.1444\n",
      "220/606 - Running Loss: 0.5996\n",
      "225/606 - Running Loss: 0.1204\n",
      "230/606 - Running Loss: 0.1454\n",
      "235/606 - Running Loss: 0.1297\n",
      "240/606 - Running Loss: 0.5921\n",
      "245/606 - Running Loss: 0.5491\n",
      "250/606 - Running Loss: 0.1816\n",
      "255/606 - Running Loss: 0.8293\n",
      "260/606 - Running Loss: 0.4455\n",
      "265/606 - Running Loss: 0.1745\n",
      "270/606 - Running Loss: 0.1627\n",
      "275/606 - Running Loss: 0.1782\n",
      "280/606 - Running Loss: 1.0352\n",
      "285/606 - Running Loss: 0.3718\n",
      "290/606 - Running Loss: 0.5024\n",
      "295/606 - Running Loss: 0.1642\n",
      "300/606 - Running Loss: 0.1902\n",
      "305/606 - Running Loss: 0.1777\n",
      "310/606 - Running Loss: 0.5021\n",
      "315/606 - Running Loss: 0.2152\n",
      "320/606 - Running Loss: 0.2052\n",
      "325/606 - Running Loss: 0.5041\n",
      "330/606 - Running Loss: 0.2218\n",
      "335/606 - Running Loss: 0.1839\n",
      "340/606 - Running Loss: 0.1874\n",
      "345/606 - Running Loss: 0.3690\n",
      "350/606 - Running Loss: 0.1643\n",
      "355/606 - Running Loss: 0.2253\n",
      "360/606 - Running Loss: 0.1917\n",
      "365/606 - Running Loss: 0.9960\n",
      "370/606 - Running Loss: 0.1418\n",
      "375/606 - Running Loss: 0.4757\n",
      "380/606 - Running Loss: 0.4757\n",
      "385/606 - Running Loss: 0.5759\n",
      "390/606 - Running Loss: 0.1245\n",
      "395/606 - Running Loss: 0.1519\n",
      "400/606 - Running Loss: 0.1690\n",
      "405/606 - Running Loss: 0.1493\n",
      "410/606 - Running Loss: 0.4240\n",
      "415/606 - Running Loss: 0.2055\n",
      "420/606 - Running Loss: 0.1761\n",
      "425/606 - Running Loss: 0.1720\n",
      "430/606 - Running Loss: 0.3339\n",
      "435/606 - Running Loss: 0.5682\n",
      "440/606 - Running Loss: 0.1429\n",
      "445/606 - Running Loss: 0.1267\n",
      "450/606 - Running Loss: 0.1191\n",
      "455/606 - Running Loss: 0.0958\n",
      "460/606 - Running Loss: 0.6824\n",
      "465/606 - Running Loss: 0.1346\n",
      "470/606 - Running Loss: 0.4630\n",
      "475/606 - Running Loss: 0.1703\n",
      "480/606 - Running Loss: 0.1727\n",
      "485/606 - Running Loss: 0.1871\n",
      "490/606 - Running Loss: 0.6064\n",
      "495/606 - Running Loss: 0.3842\n",
      "500/606 - Running Loss: 0.3138\n",
      "505/606 - Running Loss: 0.2393\n",
      "510/606 - Running Loss: 0.1369\n",
      "515/606 - Running Loss: 0.1681\n",
      "520/606 - Running Loss: 0.1083\n",
      "525/606 - Running Loss: 0.7630\n",
      "530/606 - Running Loss: 0.2924\n",
      "535/606 - Running Loss: 0.6624\n",
      "540/606 - Running Loss: 0.5896\n",
      "545/606 - Running Loss: 0.1682\n",
      "550/606 - Running Loss: 0.5376\n",
      "555/606 - Running Loss: 0.4798\n",
      "560/606 - Running Loss: 0.1487\n",
      "565/606 - Running Loss: 0.2510\n",
      "570/606 - Running Loss: 0.5588\n",
      "575/606 - Running Loss: 0.4821\n",
      "580/606 - Running Loss: 0.4220\n",
      "585/606 - Running Loss: 0.1456\n",
      "590/606 - Running Loss: 0.3237\n",
      "595/606 - Running Loss: 0.7461\n",
      "600/606 - Running Loss: 0.1525\n",
      "605/606 - Running Loss: 0.5785\n",
      "5/20 - Train Loss: 0.0010\n",
      "0/606 - Running Loss: 0.5494\n",
      "5/606 - Running Loss: 0.4526\n",
      "10/606 - Running Loss: 0.5549\n",
      "15/606 - Running Loss: 1.0068\n",
      "20/606 - Running Loss: 0.2384\n",
      "25/606 - Running Loss: 0.4035\n",
      "30/606 - Running Loss: 0.1913\n",
      "35/606 - Running Loss: 0.1718\n",
      "40/606 - Running Loss: 0.1770\n",
      "45/606 - Running Loss: 0.4431\n",
      "50/606 - Running Loss: 0.2028\n",
      "55/606 - Running Loss: 0.1501\n",
      "60/606 - Running Loss: 0.1763\n",
      "65/606 - Running Loss: 0.1250\n",
      "70/606 - Running Loss: 0.5692\n",
      "75/606 - Running Loss: 0.4159\n",
      "80/606 - Running Loss: 0.1470\n",
      "85/606 - Running Loss: 0.5288\n",
      "90/606 - Running Loss: 0.1817\n",
      "95/606 - Running Loss: 0.1158\n",
      "100/606 - Running Loss: 0.1331\n",
      "105/606 - Running Loss: 0.1038\n",
      "110/606 - Running Loss: 0.4285\n",
      "115/606 - Running Loss: 0.5212\n",
      "120/606 - Running Loss: 0.5410\n",
      "125/606 - Running Loss: 0.5512\n",
      "130/606 - Running Loss: 0.6370\n",
      "135/606 - Running Loss: 0.1327\n",
      "140/606 - Running Loss: 0.1629\n",
      "145/606 - Running Loss: 0.2005\n",
      "150/606 - Running Loss: 0.6229\n",
      "155/606 - Running Loss: 0.1509\n",
      "160/606 - Running Loss: 0.1288\n",
      "165/606 - Running Loss: 0.1209\n",
      "170/606 - Running Loss: 0.1210\n",
      "175/606 - Running Loss: 0.5684\n",
      "180/606 - Running Loss: 0.1830\n",
      "185/606 - Running Loss: 0.1911\n",
      "190/606 - Running Loss: 0.1844\n",
      "195/606 - Running Loss: 0.1423\n",
      "200/606 - Running Loss: 0.1868\n",
      "205/606 - Running Loss: 0.5882\n",
      "210/606 - Running Loss: 0.5453\n",
      "215/606 - Running Loss: 1.0356\n",
      "220/606 - Running Loss: 0.5333\n",
      "225/606 - Running Loss: 0.2013\n",
      "230/606 - Running Loss: 0.1622\n",
      "235/606 - Running Loss: 0.1851\n",
      "240/606 - Running Loss: 0.1668\n",
      "245/606 - Running Loss: 0.2115\n",
      "250/606 - Running Loss: 0.6818\n",
      "255/606 - Running Loss: 0.3672\n",
      "260/606 - Running Loss: 0.2541\n",
      "265/606 - Running Loss: 0.9205\n",
      "270/606 - Running Loss: 0.3070\n",
      "275/606 - Running Loss: 0.1301\n",
      "280/606 - Running Loss: 0.7214\n",
      "285/606 - Running Loss: 0.1528\n",
      "290/606 - Running Loss: 0.1646\n",
      "295/606 - Running Loss: 0.1249\n",
      "300/606 - Running Loss: 0.4577\n",
      "305/606 - Running Loss: 0.1087\n",
      "310/606 - Running Loss: 0.0983\n",
      "315/606 - Running Loss: 0.6183\n",
      "320/606 - Running Loss: 0.8021\n",
      "325/606 - Running Loss: 0.4077\n",
      "330/606 - Running Loss: 0.1482\n",
      "335/606 - Running Loss: 0.1497\n",
      "340/606 - Running Loss: 0.1412\n",
      "345/606 - Running Loss: 0.7443\n",
      "350/606 - Running Loss: 0.6894\n",
      "355/606 - Running Loss: 0.1798\n",
      "360/606 - Running Loss: 0.3620\n",
      "365/606 - Running Loss: 0.2379\n",
      "370/606 - Running Loss: 0.1371\n",
      "375/606 - Running Loss: 0.1315\n",
      "380/606 - Running Loss: 0.5527\n",
      "385/606 - Running Loss: 1.6051\n",
      "390/606 - Running Loss: 0.5113\n",
      "395/606 - Running Loss: 0.4791\n",
      "400/606 - Running Loss: 0.1183\n",
      "405/606 - Running Loss: 0.1593\n",
      "410/606 - Running Loss: 0.1639\n",
      "415/606 - Running Loss: 0.1370\n",
      "420/606 - Running Loss: 0.1712\n",
      "425/606 - Running Loss: 0.6158\n",
      "430/606 - Running Loss: 0.1314\n",
      "435/606 - Running Loss: 0.5164\n",
      "440/606 - Running Loss: 0.1293\n",
      "445/606 - Running Loss: 0.7195\n",
      "450/606 - Running Loss: 0.5684\n",
      "455/606 - Running Loss: 0.7498\n",
      "460/606 - Running Loss: 0.4320\n",
      "465/606 - Running Loss: 0.3420\n",
      "470/606 - Running Loss: 0.2081\n",
      "475/606 - Running Loss: 0.2973\n",
      "480/606 - Running Loss: 0.1516\n",
      "485/606 - Running Loss: 0.1508\n",
      "490/606 - Running Loss: 0.1488\n",
      "495/606 - Running Loss: 0.7904\n",
      "500/606 - Running Loss: 0.1593\n",
      "505/606 - Running Loss: 1.3538\n",
      "510/606 - Running Loss: 0.3819\n",
      "515/606 - Running Loss: 0.1492\n",
      "520/606 - Running Loss: 0.3814\n",
      "525/606 - Running Loss: 0.1419\n",
      "530/606 - Running Loss: 0.1736\n",
      "535/606 - Running Loss: 0.5924\n",
      "540/606 - Running Loss: 0.4482\n",
      "545/606 - Running Loss: 0.1865\n",
      "550/606 - Running Loss: 0.1317\n",
      "555/606 - Running Loss: 0.1488\n",
      "560/606 - Running Loss: 0.8054\n",
      "565/606 - Running Loss: 0.3760\n",
      "570/606 - Running Loss: 0.1029\n",
      "575/606 - Running Loss: 0.5120\n",
      "580/606 - Running Loss: 0.1280\n",
      "585/606 - Running Loss: 0.1316\n",
      "590/606 - Running Loss: 0.4516\n",
      "595/606 - Running Loss: 0.5470\n",
      "600/606 - Running Loss: 0.2354\n",
      "605/606 - Running Loss: 0.5222\n",
      "6/20 - Train Loss: 0.0009\n",
      "0/606 - Running Loss: 0.6965\n",
      "5/606 - Running Loss: 0.3378\n",
      "10/606 - Running Loss: 0.1765\n",
      "15/606 - Running Loss: 0.6211\n",
      "20/606 - Running Loss: 0.1488\n",
      "25/606 - Running Loss: 0.1668\n",
      "30/606 - Running Loss: 0.1414\n",
      "35/606 - Running Loss: 0.7418\n",
      "40/606 - Running Loss: 0.1188\n",
      "45/606 - Running Loss: 0.4577\n",
      "50/606 - Running Loss: 0.4676\n",
      "55/606 - Running Loss: 0.2641\n",
      "60/606 - Running Loss: 0.2191\n",
      "65/606 - Running Loss: 0.2150\n",
      "70/606 - Running Loss: 0.1902\n",
      "75/606 - Running Loss: 0.4732\n",
      "80/606 - Running Loss: 0.6577\n",
      "85/606 - Running Loss: 0.1252\n",
      "90/606 - Running Loss: 0.4861\n",
      "95/606 - Running Loss: 0.8063\n",
      "100/606 - Running Loss: 0.3875\n",
      "105/606 - Running Loss: 0.2542\n",
      "110/606 - Running Loss: 0.4746\n",
      "115/606 - Running Loss: 0.1666\n",
      "120/606 - Running Loss: 0.5257\n",
      "125/606 - Running Loss: 0.4483\n",
      "130/606 - Running Loss: 0.4637\n",
      "135/606 - Running Loss: 0.5527\n",
      "140/606 - Running Loss: 0.1645\n",
      "145/606 - Running Loss: 0.1680\n",
      "150/606 - Running Loss: 0.1488\n",
      "155/606 - Running Loss: 0.1231\n",
      "160/606 - Running Loss: 0.3283\n",
      "165/606 - Running Loss: 0.1349\n",
      "170/606 - Running Loss: 0.4312\n",
      "175/606 - Running Loss: 0.1107\n",
      "180/606 - Running Loss: 0.2030\n",
      "185/606 - Running Loss: 0.1257\n",
      "190/606 - Running Loss: 0.1355\n",
      "195/606 - Running Loss: 0.2379\n",
      "200/606 - Running Loss: 0.5444\n",
      "205/606 - Running Loss: 0.5659\n",
      "210/606 - Running Loss: 0.5595\n",
      "215/606 - Running Loss: 0.2066\n",
      "220/606 - Running Loss: 0.1768\n",
      "225/606 - Running Loss: 0.4156\n",
      "230/606 - Running Loss: 0.2069\n",
      "235/606 - Running Loss: 0.1375\n",
      "240/606 - Running Loss: 0.2762\n",
      "245/606 - Running Loss: 0.3767\n",
      "250/606 - Running Loss: 0.8288\n",
      "255/606 - Running Loss: 0.1442\n",
      "260/606 - Running Loss: 0.5516\n",
      "265/606 - Running Loss: 0.6776\n",
      "270/606 - Running Loss: 0.1651\n",
      "275/606 - Running Loss: 0.7625\n",
      "280/606 - Running Loss: 0.1220\n",
      "285/606 - Running Loss: 0.1601\n",
      "290/606 - Running Loss: 0.1339\n",
      "295/606 - Running Loss: 0.6570\n",
      "300/606 - Running Loss: 0.7743\n",
      "305/606 - Running Loss: 0.1363\n",
      "310/606 - Running Loss: 0.6985\n",
      "315/606 - Running Loss: 0.1496\n",
      "320/606 - Running Loss: 0.1010\n",
      "325/606 - Running Loss: 0.5640\n",
      "330/606 - Running Loss: 0.1547\n",
      "335/606 - Running Loss: 0.1640\n",
      "340/606 - Running Loss: 0.1635\n",
      "345/606 - Running Loss: 0.3163\n",
      "350/606 - Running Loss: 0.0994\n",
      "355/606 - Running Loss: 0.4255\n",
      "360/606 - Running Loss: 0.1664\n",
      "365/606 - Running Loss: 0.1294\n",
      "370/606 - Running Loss: 0.5842\n",
      "375/606 - Running Loss: 0.2693\n",
      "380/606 - Running Loss: 0.4851\n",
      "385/606 - Running Loss: 0.7438\n",
      "390/606 - Running Loss: 0.6662\n",
      "395/606 - Running Loss: 0.5296\n",
      "400/606 - Running Loss: 0.5673\n",
      "405/606 - Running Loss: 0.1347\n",
      "410/606 - Running Loss: 0.3739\n",
      "415/606 - Running Loss: 0.1502\n",
      "420/606 - Running Loss: 0.2256\n",
      "425/606 - Running Loss: 0.4645\n",
      "430/606 - Running Loss: 0.9453\n",
      "435/606 - Running Loss: 0.4885\n",
      "440/606 - Running Loss: 0.6619\n",
      "445/606 - Running Loss: 0.4754\n",
      "450/606 - Running Loss: 0.5307\n",
      "455/606 - Running Loss: 0.4214\n",
      "460/606 - Running Loss: 0.2183\n",
      "465/606 - Running Loss: 0.2028\n",
      "470/606 - Running Loss: 0.1785\n",
      "475/606 - Running Loss: 0.1734\n",
      "480/606 - Running Loss: 0.1301\n",
      "485/606 - Running Loss: 0.1551\n",
      "490/606 - Running Loss: 0.4123\n",
      "495/606 - Running Loss: 0.9820\n",
      "500/606 - Running Loss: 0.1068\n",
      "505/606 - Running Loss: 0.1132\n",
      "510/606 - Running Loss: 0.2914\n",
      "515/606 - Running Loss: 0.1395\n",
      "520/606 - Running Loss: 0.1406\n",
      "525/606 - Running Loss: 0.7691\n",
      "530/606 - Running Loss: 0.5509\n",
      "535/606 - Running Loss: 0.3636\n",
      "540/606 - Running Loss: 0.1200\n",
      "545/606 - Running Loss: 0.1291\n",
      "550/606 - Running Loss: 0.1833\n",
      "555/606 - Running Loss: 0.1243\n",
      "560/606 - Running Loss: 0.1466\n",
      "565/606 - Running Loss: 0.6714\n",
      "570/606 - Running Loss: 0.2486\n",
      "575/606 - Running Loss: 0.2181\n",
      "580/606 - Running Loss: 0.5009\n",
      "585/606 - Running Loss: 0.1717\n",
      "590/606 - Running Loss: 0.3668\n",
      "595/606 - Running Loss: 0.5432\n",
      "600/606 - Running Loss: 0.4366\n",
      "605/606 - Running Loss: 0.7048\n",
      "7/20 - Train Loss: 0.0012\n",
      "0/606 - Running Loss: 0.4023\n",
      "5/606 - Running Loss: 0.1795\n",
      "10/606 - Running Loss: 0.3400\n",
      "15/606 - Running Loss: 0.2392\n",
      "20/606 - Running Loss: 0.1924\n",
      "25/606 - Running Loss: 0.1597\n",
      "30/606 - Running Loss: 0.5190\n",
      "35/606 - Running Loss: 0.1814\n",
      "40/606 - Running Loss: 0.1542\n",
      "45/606 - Running Loss: 0.2112\n",
      "50/606 - Running Loss: 0.2078\n",
      "55/606 - Running Loss: 0.1334\n",
      "60/606 - Running Loss: 0.1637\n",
      "65/606 - Running Loss: 0.1536\n",
      "70/606 - Running Loss: 0.1431\n",
      "75/606 - Running Loss: 0.4877\n",
      "80/606 - Running Loss: 0.1195\n",
      "85/606 - Running Loss: 0.2459\n",
      "90/606 - Running Loss: 0.6849\n",
      "95/606 - Running Loss: 0.0980\n",
      "100/606 - Running Loss: 0.5661\n",
      "105/606 - Running Loss: 0.1395\n",
      "110/606 - Running Loss: 0.7671\n",
      "115/606 - Running Loss: 0.8454\n",
      "120/606 - Running Loss: 0.1152\n",
      "125/606 - Running Loss: 0.8243\n",
      "130/606 - Running Loss: 0.1231\n",
      "135/606 - Running Loss: 0.1456\n",
      "140/606 - Running Loss: 0.1584\n",
      "145/606 - Running Loss: 0.5311\n",
      "150/606 - Running Loss: 0.2384\n",
      "155/606 - Running Loss: 0.8011\n",
      "160/606 - Running Loss: 0.4493\n",
      "165/606 - Running Loss: 0.4252\n",
      "170/606 - Running Loss: 0.2000\n",
      "175/606 - Running Loss: 0.3610\n",
      "180/606 - Running Loss: 0.5239\n",
      "185/606 - Running Loss: 0.2205\n",
      "190/606 - Running Loss: 0.1503\n",
      "195/606 - Running Loss: 0.1526\n",
      "200/606 - Running Loss: 0.4999\n",
      "205/606 - Running Loss: 0.1286\n",
      "210/606 - Running Loss: 0.6057\n",
      "215/606 - Running Loss: 0.1938\n",
      "220/606 - Running Loss: 0.1094\n",
      "225/606 - Running Loss: 0.2248\n",
      "230/606 - Running Loss: 0.1169\n",
      "235/606 - Running Loss: 0.1724\n",
      "240/606 - Running Loss: 0.1077\n",
      "245/606 - Running Loss: 0.4337\n",
      "250/606 - Running Loss: 0.1578\n",
      "255/606 - Running Loss: 0.1026\n",
      "260/606 - Running Loss: 0.1799\n",
      "265/606 - Running Loss: 0.6062\n",
      "270/606 - Running Loss: 0.0952\n",
      "275/606 - Running Loss: 0.6339\n",
      "280/606 - Running Loss: 0.0950\n",
      "285/606 - Running Loss: 0.1593\n",
      "290/606 - Running Loss: 0.6126\n",
      "295/606 - Running Loss: 0.5774\n",
      "300/606 - Running Loss: 0.1453\n",
      "305/606 - Running Loss: 0.3303\n",
      "310/606 - Running Loss: 0.2422\n",
      "315/606 - Running Loss: 0.7281\n",
      "320/606 - Running Loss: 0.1789\n",
      "325/606 - Running Loss: 0.6769\n",
      "330/606 - Running Loss: 0.1306\n",
      "335/606 - Running Loss: 0.6608\n",
      "340/606 - Running Loss: 0.1542\n",
      "345/606 - Running Loss: 0.4400\n",
      "350/606 - Running Loss: 0.1321\n",
      "355/606 - Running Loss: 0.2359\n",
      "360/606 - Running Loss: 0.5372\n",
      "365/606 - Running Loss: 0.1127\n",
      "370/606 - Running Loss: 0.5649\n",
      "375/606 - Running Loss: 0.1380\n",
      "380/606 - Running Loss: 0.5613\n",
      "385/606 - Running Loss: 0.1340\n",
      "390/606 - Running Loss: 0.4678\n",
      "395/606 - Running Loss: 0.2795\n",
      "400/606 - Running Loss: 0.5695\n",
      "405/606 - Running Loss: 0.1478\n",
      "410/606 - Running Loss: 0.1716\n",
      "415/606 - Running Loss: 0.1511\n",
      "420/606 - Running Loss: 0.2143\n",
      "425/606 - Running Loss: 0.5584\n",
      "430/606 - Running Loss: 0.4486\n",
      "435/606 - Running Loss: 0.3620\n",
      "440/606 - Running Loss: 0.2422\n",
      "445/606 - Running Loss: 0.2895\n",
      "450/606 - Running Loss: 0.1487\n",
      "455/606 - Running Loss: 0.4781\n",
      "460/606 - Running Loss: 0.1503\n",
      "465/606 - Running Loss: 0.3876\n",
      "470/606 - Running Loss: 0.3207\n",
      "475/606 - Running Loss: 0.3746\n",
      "480/606 - Running Loss: 0.5712\n",
      "485/606 - Running Loss: 0.1250\n",
      "490/606 - Running Loss: 0.1799\n",
      "495/606 - Running Loss: 0.5842\n",
      "500/606 - Running Loss: 0.3933\n",
      "505/606 - Running Loss: 0.1675\n",
      "510/606 - Running Loss: 0.6196\n",
      "515/606 - Running Loss: 0.2133\n",
      "520/606 - Running Loss: 0.2027\n",
      "525/606 - Running Loss: 0.4866\n",
      "530/606 - Running Loss: 0.5118\n",
      "535/606 - Running Loss: 0.6892\n",
      "540/606 - Running Loss: 0.7296\n",
      "545/606 - Running Loss: 0.1516\n",
      "550/606 - Running Loss: 0.4471\n",
      "555/606 - Running Loss: 0.1786\n",
      "560/606 - Running Loss: 0.8722\n",
      "565/606 - Running Loss: 0.5295\n",
      "570/606 - Running Loss: 0.2470\n",
      "575/606 - Running Loss: 0.1635\n",
      "580/606 - Running Loss: 0.1406\n",
      "585/606 - Running Loss: 0.1965\n",
      "590/606 - Running Loss: 0.2053\n",
      "595/606 - Running Loss: 0.4663\n",
      "600/606 - Running Loss: 0.4239\n",
      "605/606 - Running Loss: 0.1080\n",
      "8/20 - Train Loss: 0.0002\n",
      "0/606 - Running Loss: 0.1013\n",
      "5/606 - Running Loss: 0.3721\n",
      "10/606 - Running Loss: 0.1030\n",
      "15/606 - Running Loss: 0.1409\n",
      "20/606 - Running Loss: 0.5998\n",
      "25/606 - Running Loss: 0.3579\n",
      "30/606 - Running Loss: 0.4895\n",
      "35/606 - Running Loss: 0.5990\n",
      "40/606 - Running Loss: 0.1682\n",
      "45/606 - Running Loss: 0.1606\n",
      "50/606 - Running Loss: 0.1427\n",
      "55/606 - Running Loss: 0.3307\n",
      "60/606 - Running Loss: 0.1300\n",
      "65/606 - Running Loss: 0.3944\n",
      "70/606 - Running Loss: 0.3962\n",
      "75/606 - Running Loss: 0.4982\n",
      "80/606 - Running Loss: 0.4995\n",
      "85/606 - Running Loss: 0.1940\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:9\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(trainloader, model, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfor\u001b[39;00m idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader):\n\u001b[1;32m     14\u001b[0m     train_batch_data \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m     target_batch \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/unet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/unet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/unet/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/unet/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[4], line 95\u001b[0m, in \u001b[0;36mCT_Dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m# total = np.zeros((388, 388))\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(labels)):\n\u001b[0;32m---> 95\u001b[0m     total \u001b[39m=\u001b[39m total \u001b[39m+\u001b[39m labels[i]\n\u001b[1;32m     97\u001b[0m total\u001b[39m=\u001b[39m (total\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[39m# total = total.reshape(((1, 520, 704)))\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = EPOCHS\n",
    "\n",
    "best_epoch = 0\n",
    "best_score = 0.0\n",
    "train_loss = []\n",
    "# val_loss, val_dice_coefficient = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = train_one_epoch(trainloader, model, optimizer, criterion, DEVICE)\n",
    "    train_loss.append(losses[\"train\"])\n",
    "    # val_loss.append(losses[\"val\"])\n",
    "    \n",
    "    print(f\"{epoch}/{num_epochs} - Train Loss: {losses['train']:.4f}\")\n",
    "    \n",
    "    if epoch % 2 ==0:\n",
    "        save_model(model.state_dict(), f\"model_{epoch:02d}.pth\")\n",
    "        \n",
    "print(f\"Best epoch: {best_epoch} -> Best Dice Coeffient: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88837590-f157-4479-9ef0-08ec7e27a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. weight 저장후, inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee27ead-84fa-4902-a91a-095189fd055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9. 평가지표로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807919ee-3355-4cf2-a1f9-cf8e98ed2bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f9a62-2684-4cb7-963e-1e49a4674a25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
