{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73dea87-08a4-4601-94c5-bd71c7f94b5e",
   "metadata": {},
   "source": [
    "### 1. 모델 구현\n",
    "### 2. 데이터 클래스\n",
    "### 3. 데이터 로더\n",
    "### 4. 손실함수\n",
    "### 5. optimizer\n",
    "### 6. weight 저장후, inference\n",
    "### 7. 평가지표로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd85c93d-7928-4515-babd-711c88a6fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. 모델 구현 OK\n",
    "# 직사각형 input, 직사각형 output 되도록 kernel사이즈나 그런것들 조절해주기\n",
    "### 2. 데이터 클래스 @\n",
    "#  run length encoded된거 decode하기 @\n",
    "### 3. 데이터 로더 @\n",
    "### 4. Transformer, collate_fn 구현 -----> Proceeding...\n",
    "# 데이터 클래스 이전에 구현해서 데이터 인스턴스에 넣어주기\n",
    "### 5. 손실함수\n",
    "### 6. optimizer\n",
    "### 7. weight 저장후, inference\n",
    "### 8. 평가지표로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59bfcc53-0d4c-486a-b41c-c151fb673221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "caa45a44-3048-4108-afc3-c3b7e2472c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.모델구현\n",
    "class Unet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.ConPathLayer1 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.ConPathLayer2 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.ConPathLayer3 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.ConPathLayer4 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.MaxPool4 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.BottomLayer = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv1 = torch.nn.ConvTranspose2d(in_channels=1024, out_channels=512,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer1 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv2 = torch.nn.ConvTranspose2d(in_channels=512, out_channels=256,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer2 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv3 = torch.nn.ConvTranspose2d(in_channels=256, out_channels=128,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer3 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.UpConv4 = torch.nn.ConvTranspose2d(in_channels=128, out_channels=64,kernel_size=2 , stride=2 )\n",
    "\n",
    "    self.ExpPathLayer4 = nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1, stride=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.ConPathLayer1(x)\n",
    "    skip1 = torchvision.transforms.CenterCrop(392)(x)\n",
    "    x = self.MaxPool1(x)\n",
    "    x = self.ConPathLayer2(x)\n",
    "    skip2 = torchvision.transforms.CenterCrop(200)(x)\n",
    "    x = self.MaxPool2(x)\n",
    "    x = self.ConPathLayer3(x)\n",
    "    skip3 = torchvision.transforms.CenterCrop(104)(x)\n",
    "    x = self.MaxPool3(x)\n",
    "    x = self.ConPathLayer4(x)\n",
    "    skip4 = torchvision.transforms.CenterCrop(56)(x)\n",
    "    x = self.MaxPool4(x)\n",
    "    # print(\"skip1:{}, skip2:{}, skip3:{}, skip4:{}\".format(skip1.shape, skip2.shape, skip3.shape, skip4.shape))\n",
    "    x = self.BottomLayer(x)\n",
    "    x = self.UpConv1(x)\n",
    "    x = torch.cat((skip4, x), dim=1)\n",
    "    x = self.ExpPathLayer1(x)\n",
    "    x = self.UpConv2(x)\n",
    "    x = torch.cat((skip3, x), dim=1)\n",
    "    x = self.ExpPathLayer2(x)\n",
    "    x = self.UpConv3(x)\n",
    "    x = torch.cat((skip2, x), dim=1)\n",
    "    x = self.ExpPathLayer3(x)\n",
    "    x = self.UpConv4(x)\n",
    "    x = torch.cat((skip1, x), dim=1)\n",
    "    result = self.ExpPathLayer4(x)\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "74bd23fd-79da-4485-be34-25fb697da161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 데이터 클래스 정의 \n",
    "class CT_Dataset():\n",
    "    def __init__(self, phase, transformer=None, target_transformer=None):\n",
    "        # self.train_csv_path = \"/content/drive/MyDrive/Paper Implementation/Unet/dataset/train/train.csv\"\n",
    "        # self.train_path = \"/content/drive/MyDrive/Paper Implementation/Unet/dataset/train\"\n",
    "        self.train_csv_path = \"C:/Users/syb62/Desktop/Unet/train/train.csv\"\n",
    "        self.train_path = \"C:/Users/syb62/Desktop/Unet/train\"\n",
    "        \n",
    "        self.phase = phase\n",
    "        self.train_img_list = [] # train 이미지들 목록\n",
    "        self.image_info = collections.defaultdict(dict) # train이미지 id, 이미지 경로, 라벨(annotation)\n",
    "        self.transformer = transformer\n",
    "        self.target_transformer = target_transformer\n",
    "        \n",
    "        ################ train이미지 목록 리스트 생성 ################\n",
    "        train_list = os.listdir(self.train_path)\n",
    "        for i in train_list:\n",
    "          if i.endswith(\".png\"):\n",
    "            self.train_img_list.append(i)\n",
    "\n",
    "        ################ image_info 생성 ################\n",
    "        train_df = pd.read_csv(self.train_csv_path)\n",
    "        anno_list=[]\n",
    "        df_ids = np.array(train_df.loc[:, \"id\"].values).tolist()\n",
    "        mask_dic = {}\n",
    "\n",
    "        for img_name in self.train_img_list:\n",
    "          id = img_name.split(\".\")[0]\n",
    "          anno_list=[]\n",
    "          for j in range(len(df_ids)):\n",
    "            if df_ids[j] == id:\n",
    "              a = train_df.loc[:, \"annotation\"].values[j]\n",
    "              anno_list.append(a)\n",
    "            else:\n",
    "              pass\n",
    "          mask_dic[id]=anno_list\n",
    "\n",
    "          for i in range(len(mask_dic.keys())):\n",
    "            mask_dic_keys = list(mask_dic.keys())\n",
    "            self.image_info[i]={\n",
    "                    'image_id': mask_dic_keys[i],\n",
    "                    'image_path': os.path.join(self.train_path, mask_dic_keys[i] + '.png'),\n",
    "                    'annotations': mask_dic[mask_dic_keys[i]]\n",
    "                    }\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.train_img_list)\n",
    "    \n",
    "    def rle_decode(self, mask_rle, shape, color=1):\n",
    "        '''\n",
    "        mask_rle: run-length as string formated (start length)\n",
    "        shape: (height,width) of array to return \n",
    "        Returns numpy array, 1 - mask, 0 - background\n",
    "        '''\n",
    "        s = mask_rle.split()\n",
    "        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "        starts -= 1\n",
    "        ends = starts + lengths\n",
    "        img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n",
    "        for lo, hi in zip(starts, ends):\n",
    "            img[lo : hi] = color\n",
    "        return img.reshape(shape)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        info = self.image_info[idx]\n",
    "        img_path = info[\"image_path\"]\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        ##########################\n",
    "\n",
    "        HEIGHT = 520\n",
    "        WIDTH = 704\n",
    "        # HEIGHT = 388\n",
    "        # WIDTH = 388\n",
    "        height = HEIGHT\n",
    "        width = WIDTH\n",
    "\n",
    "        masks = np.zeros((len(info['annotations']), height, width), dtype=np.uint8)\n",
    "        labels=[]\n",
    "\n",
    "        for i, annotation in enumerate(info['annotations']):\n",
    "            a_mask = self.rle_decode(annotation, (HEIGHT, WIDTH))\n",
    "            a_mask = Image.fromarray(a_mask)\n",
    "\n",
    "            # if self.should_resize:\n",
    "            if True:\n",
    "                a_mask = a_mask.resize((width, height), resample=Image.BILINEAR)\n",
    "                a_mask = np.array(a_mask) > 0\n",
    "                labels.append(a_mask)\n",
    "                masks[i, :, :] = a_mask\n",
    "        ##########################\n",
    "        total = np.zeros((520, 704))\n",
    "        # total = np.zeros((388, 388))\n",
    "        for i in range(len(labels)):\n",
    "            total = total + labels[i]\n",
    "        \n",
    "        total= (total>=1)\n",
    "        # total = total.reshape(((1, 520, 704)))\n",
    "        target = total\n",
    "        \n",
    "        if self.transformer:\n",
    "            image = self.transformer(image)\n",
    "        \n",
    "        if self.target_transformer:\n",
    "            target_trans = self.target_transformer(target)\n",
    "        \n",
    "        # target = torch.from_numpy(target).long()\n",
    "        target = target_trans[0,:,:].long()\n",
    "        \n",
    "        \n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "445ca2b8-3112-4bfd-b571-ff66e49ecd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Transformer & collate_fn 구현\n",
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(572,572))\n",
    "    # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    \n",
    "])\n",
    "\n",
    "target_transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(388,388))\n",
    "    # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb69e02f-1b2f-48b2-813c-7c947e02c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for a, b in batch: \n",
    "        images.append(a)\n",
    "        targets.append(b)\n",
    "    images = torch.stack(images, dim=0) \n",
    "    targets = torch.stack(targets, dim=0)\n",
    "\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cb06b97b-03d4-4a41-9d45-0638f13b4617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syb62\\anaconda3\\envs\\unet\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_data = CT_Dataset(phase='train', transformer=transformer, target_transformer=target_transformer)\n",
    "data, target = train_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a8931f2c-1727-435a-ac81-fa2d86608a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(target, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9b8a05f1-5f87-4b19-af36-a582855f8dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. 손실함수 계산 Class 선언\n",
    "class Unet_loss():\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes=num_classes\n",
    "        self.CE_loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        # [B, 2, 520, 704] 크기의 prediction입력\n",
    "        # self.prediction=prediction\n",
    "        # [B, 520, 704] 크기의 target입력\n",
    "        # self.target=target\n",
    "    def __call__(self, prediction, target):\n",
    "        loss = self.get_loss(prediction, target)\n",
    "        return loss\n",
    "    \n",
    "    def get_loss(self,  prediction, target):\n",
    "        ################ Dice Coefficient 적용할거면 연산 진행해서 넣어주기 ################\n",
    "        # prediction: [B, 2, 520, 704] -> [B, 2, 520, 704] 예측한 label값으로 변환\n",
    "        predictions_ = torch.argmax(prediction, dim=1)\n",
    "        onehot_pred = F.one_hot(predictions_,num_classes=self.num_classes)\n",
    "        onehot_pred = onehot_pred.permute(0,3,1,2)\n",
    "        \n",
    "        # prediction: [B, 520, 704] -> [B, 2, 520, 704] 을 예측한 label값으로 변환\n",
    "        onehot_target = F.one_hot(target , num_classes=self.num_classes).permute(0,3,1,2)\n",
    "        ########################################################################################\n",
    "        \n",
    "        # CE loss 계산\n",
    "        # ce_loss = self.CE_loss(onehot_pred, onehot_target)\n",
    "        ce_loss = self.CE_loss(prediction, target)\n",
    "        \n",
    "        return ce_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3c2d6f91-9f3a-41c1-aa82-8d46d06be89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Training\n",
    "def train_one_epoch(trainloader, model,optimizer,criterion,device):\n",
    "    losses = {}\n",
    "    # for phase in ['train', 'val']:\n",
    "    for phase in ['train']:\n",
    "        running_loss = 0.0\n",
    "\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        for idx, batch in enumerate(trainloader):\n",
    "            train_batch_data = batch[0].to(device)\n",
    "            target_batch = batch[1].to(device)\n",
    "            \n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                pred = model(train_batch_data)\n",
    "                loss = criterion(pred,target_batch)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                optimizer.zero_grad() \n",
    "                # optimizer의 갱신할 기울기 값을, 즉, 연결되어있는 모든 model params의 기울기값들을 초기화\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss = loss.item()\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                if idx % 5 == 0:\n",
    "                    text = f\"{idx}/{len(trainloader)}\" + \\\n",
    "                            f\" - Running Loss: {loss.item():.4f}\"\n",
    "                    print(text)\n",
    "            \n",
    "        losses[phase] = running_loss / len(trainloader)\n",
    "                    \n",
    "    # print(f\"idx:{idx}, batch image: {train_batch_data.shape}, \\\n",
    "    # batch target:{target_batch.shape}, output: {pred.shape}, loss: {loss}\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a9db5a28-930f-4bd6-aa3e-e22c4d56af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_state, model_name, save_dir=\"C:/Users/syb62/Desktop/Unet/models_cpu_square\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model_state, os.path.join(save_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ba5e42a-75bd-45cc-b78f-8263d9d7eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = False\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available and is_cuda else 'cpu')\n",
    "# DEVICE = torch.device('cpu')\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 20\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "train_data = CT_Dataset(phase='train', transformer=transformer, target_transformer=target_transformer)\n",
    "trainloader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn,shuffle=True)\n",
    "model = Unet()\n",
    "model = model.to(DEVICE)\n",
    "criterion = Unet_loss(num_classes=NUM_CLASSES)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= LEARNING_RATE, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "68e5303b-3f0b-40c6-9b34-dbc57ae601e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/76 - Running Loss: 0.6787\n",
      "5/76 - Running Loss: 0.6766\n",
      "10/76 - Running Loss: 0.6656\n",
      "15/76 - Running Loss: 0.6398\n",
      "20/76 - Running Loss: 0.6232\n",
      "25/76 - Running Loss: 0.6224\n",
      "30/76 - Running Loss: 0.5888\n",
      "35/76 - Running Loss: 0.5758\n",
      "40/76 - Running Loss: 0.5514\n",
      "45/76 - Running Loss: 0.5701\n",
      "50/76 - Running Loss: 0.5310\n",
      "55/76 - Running Loss: 0.4673\n",
      "60/76 - Running Loss: 0.4962\n",
      "65/76 - Running Loss: 0.5150\n",
      "70/76 - Running Loss: 0.4811\n",
      "75/76 - Running Loss: 0.4190\n",
      "0/20 - Train Loss: 0.0055\n",
      "0/76 - Running Loss: 0.5079\n",
      "5/76 - Running Loss: 0.4222\n",
      "10/76 - Running Loss: 0.3376\n",
      "15/76 - Running Loss: 0.3800\n",
      "20/76 - Running Loss: 0.3138\n",
      "25/76 - Running Loss: 0.3165\n",
      "30/76 - Running Loss: 0.2739\n",
      "35/76 - Running Loss: 0.3269\n",
      "40/76 - Running Loss: 0.5246\n",
      "45/76 - Running Loss: 0.5226\n",
      "50/76 - Running Loss: 0.3718\n",
      "55/76 - Running Loss: 0.3386\n",
      "60/76 - Running Loss: 0.3908\n",
      "65/76 - Running Loss: 0.3785\n",
      "70/76 - Running Loss: 0.4108\n",
      "75/76 - Running Loss: 0.3950\n",
      "1/20 - Train Loss: 0.0052\n",
      "0/76 - Running Loss: 0.3479\n",
      "5/76 - Running Loss: 0.2145\n",
      "10/76 - Running Loss: 0.3290\n",
      "15/76 - Running Loss: 0.3213\n",
      "20/76 - Running Loss: 0.4165\n",
      "25/76 - Running Loss: 0.3489\n",
      "30/76 - Running Loss: 0.2762\n",
      "35/76 - Running Loss: 0.3199\n",
      "40/76 - Running Loss: 0.3828\n",
      "45/76 - Running Loss: 0.3868\n",
      "50/76 - Running Loss: 0.2683\n",
      "55/76 - Running Loss: 0.3158\n",
      "60/76 - Running Loss: 0.3725\n",
      "65/76 - Running Loss: 0.5167\n",
      "70/76 - Running Loss: 0.4087\n",
      "75/76 - Running Loss: 0.2837\n",
      "2/20 - Train Loss: 0.0037\n",
      "0/76 - Running Loss: 0.2589\n",
      "5/76 - Running Loss: 0.4288\n",
      "10/76 - Running Loss: 0.1831\n",
      "15/76 - Running Loss: 0.3234\n",
      "20/76 - Running Loss: 0.3711\n",
      "25/76 - Running Loss: 0.4655\n",
      "30/76 - Running Loss: 0.3356\n",
      "35/76 - Running Loss: 0.3419\n",
      "40/76 - Running Loss: 0.3963\n",
      "45/76 - Running Loss: 0.4159\n",
      "50/76 - Running Loss: 0.2638\n",
      "55/76 - Running Loss: 0.3763\n",
      "60/76 - Running Loss: 0.3525\n",
      "65/76 - Running Loss: 0.3716\n",
      "70/76 - Running Loss: 0.5639\n",
      "75/76 - Running Loss: 0.4004\n",
      "3/20 - Train Loss: 0.0053\n",
      "0/76 - Running Loss: 0.4014\n",
      "5/76 - Running Loss: 0.4437\n",
      "10/76 - Running Loss: 0.2159\n",
      "15/76 - Running Loss: 0.2518\n",
      "20/76 - Running Loss: 0.2553\n",
      "25/76 - Running Loss: 0.2623\n",
      "30/76 - Running Loss: 0.3839\n",
      "35/76 - Running Loss: 0.4521\n",
      "40/76 - Running Loss: 0.4031\n",
      "45/76 - Running Loss: 0.3340\n",
      "50/76 - Running Loss: 0.4649\n",
      "55/76 - Running Loss: 0.2476\n",
      "60/76 - Running Loss: 0.4822\n",
      "65/76 - Running Loss: 0.3137\n",
      "70/76 - Running Loss: 0.2911\n",
      "75/76 - Running Loss: 0.4351\n",
      "4/20 - Train Loss: 0.0057\n",
      "0/76 - Running Loss: 0.3100\n",
      "5/76 - Running Loss: 0.2964\n",
      "10/76 - Running Loss: 0.3171\n",
      "15/76 - Running Loss: 0.3953\n",
      "20/76 - Running Loss: 0.4115\n",
      "25/76 - Running Loss: 0.3581\n",
      "30/76 - Running Loss: 0.3564\n",
      "35/76 - Running Loss: 0.2626\n",
      "40/76 - Running Loss: 0.2674\n",
      "45/76 - Running Loss: 0.2997\n",
      "50/76 - Running Loss: 0.2348\n",
      "55/76 - Running Loss: 0.2786\n",
      "60/76 - Running Loss: 0.3266\n",
      "65/76 - Running Loss: 0.3702\n",
      "70/76 - Running Loss: 0.3960\n",
      "75/76 - Running Loss: 0.4088\n",
      "5/20 - Train Loss: 0.0054\n",
      "0/76 - Running Loss: 0.3201\n",
      "5/76 - Running Loss: 0.4678\n",
      "10/76 - Running Loss: 0.4152\n",
      "15/76 - Running Loss: 0.3346\n",
      "20/76 - Running Loss: 0.4310\n",
      "25/76 - Running Loss: 0.3721\n",
      "30/76 - Running Loss: 0.3103\n",
      "35/76 - Running Loss: 0.3957\n",
      "40/76 - Running Loss: 0.3632\n",
      "45/76 - Running Loss: 0.3336\n",
      "50/76 - Running Loss: 0.3770\n",
      "55/76 - Running Loss: 0.4074\n",
      "60/76 - Running Loss: 0.4188\n",
      "65/76 - Running Loss: 0.2365\n",
      "70/76 - Running Loss: 0.2142\n",
      "75/76 - Running Loss: 0.4008\n",
      "6/20 - Train Loss: 0.0053\n",
      "0/76 - Running Loss: 0.3627\n",
      "5/76 - Running Loss: 0.3379\n",
      "10/76 - Running Loss: 0.4540\n",
      "15/76 - Running Loss: 0.3716\n",
      "20/76 - Running Loss: 0.3832\n",
      "25/76 - Running Loss: 0.4400\n",
      "30/76 - Running Loss: 0.3660\n",
      "35/76 - Running Loss: 0.2849\n",
      "40/76 - Running Loss: 0.3417\n",
      "45/76 - Running Loss: 0.4006\n",
      "50/76 - Running Loss: 0.3363\n",
      "55/76 - Running Loss: 0.2883\n",
      "60/76 - Running Loss: 0.4141\n",
      "65/76 - Running Loss: 0.4557\n",
      "70/76 - Running Loss: 0.4709\n",
      "75/76 - Running Loss: 0.4869\n",
      "7/20 - Train Loss: 0.0064\n",
      "0/76 - Running Loss: 0.3984\n",
      "5/76 - Running Loss: 0.4482\n",
      "10/76 - Running Loss: 0.4962\n",
      "15/76 - Running Loss: 0.4083\n",
      "20/76 - Running Loss: 0.5111\n",
      "25/76 - Running Loss: 0.3756\n",
      "30/76 - Running Loss: 0.4704\n",
      "35/76 - Running Loss: 0.2634\n",
      "40/76 - Running Loss: 0.3622\n",
      "45/76 - Running Loss: 0.3059\n",
      "50/76 - Running Loss: 0.3376\n",
      "55/76 - Running Loss: 0.3346\n",
      "60/76 - Running Loss: 0.4623\n",
      "65/76 - Running Loss: 0.2006\n",
      "70/76 - Running Loss: 0.3125\n",
      "75/76 - Running Loss: 0.4540\n",
      "8/20 - Train Loss: 0.0060\n",
      "0/76 - Running Loss: 0.3458\n",
      "5/76 - Running Loss: 0.2379\n",
      "10/76 - Running Loss: 0.3074\n",
      "15/76 - Running Loss: 0.2668\n",
      "20/76 - Running Loss: 0.3840\n",
      "25/76 - Running Loss: 0.3948\n",
      "30/76 - Running Loss: 0.4005\n",
      "35/76 - Running Loss: 0.2395\n",
      "40/76 - Running Loss: 0.3920\n",
      "45/76 - Running Loss: 0.3262\n",
      "50/76 - Running Loss: 0.4758\n",
      "55/76 - Running Loss: 0.3159\n",
      "60/76 - Running Loss: 0.2993\n",
      "65/76 - Running Loss: 0.3815\n",
      "70/76 - Running Loss: 0.4732\n",
      "75/76 - Running Loss: 0.2199\n",
      "9/20 - Train Loss: 0.0029\n",
      "0/76 - Running Loss: 0.3132\n",
      "5/76 - Running Loss: 0.4340\n",
      "10/76 - Running Loss: 0.4001\n",
      "15/76 - Running Loss: 0.2853\n",
      "20/76 - Running Loss: 0.3099\n",
      "25/76 - Running Loss: 0.3568\n",
      "30/76 - Running Loss: 0.3310\n",
      "35/76 - Running Loss: 0.4897\n",
      "40/76 - Running Loss: 0.2993\n",
      "45/76 - Running Loss: 0.3911\n",
      "50/76 - Running Loss: 0.2877\n",
      "55/76 - Running Loss: 0.4180\n",
      "60/76 - Running Loss: 0.2313\n",
      "65/76 - Running Loss: 0.3942\n",
      "70/76 - Running Loss: 0.3700\n",
      "75/76 - Running Loss: 0.2646\n",
      "10/20 - Train Loss: 0.0035\n",
      "0/76 - Running Loss: 0.2982\n",
      "5/76 - Running Loss: 0.4857\n",
      "10/76 - Running Loss: 0.3833\n",
      "15/76 - Running Loss: 0.4301\n",
      "20/76 - Running Loss: 0.3314\n",
      "25/76 - Running Loss: 0.3449\n",
      "30/76 - Running Loss: 0.2250\n",
      "35/76 - Running Loss: 0.2675\n",
      "40/76 - Running Loss: 0.3943\n",
      "45/76 - Running Loss: 0.4752\n",
      "50/76 - Running Loss: 0.3807\n",
      "55/76 - Running Loss: 0.2386\n",
      "60/76 - Running Loss: 0.5244\n",
      "65/76 - Running Loss: 0.3732\n",
      "70/76 - Running Loss: 0.4825\n",
      "75/76 - Running Loss: 0.3919\n",
      "11/20 - Train Loss: 0.0052\n",
      "0/76 - Running Loss: 0.2660\n",
      "5/76 - Running Loss: 0.2245\n",
      "10/76 - Running Loss: 0.4752\n",
      "15/76 - Running Loss: 0.3886\n",
      "20/76 - Running Loss: 0.4396\n",
      "25/76 - Running Loss: 0.4086\n",
      "30/76 - Running Loss: 0.4222\n",
      "35/76 - Running Loss: 0.2816\n",
      "40/76 - Running Loss: 0.3390\n",
      "45/76 - Running Loss: 0.2925\n",
      "50/76 - Running Loss: 0.3079\n",
      "55/76 - Running Loss: 0.2305\n",
      "60/76 - Running Loss: 0.3578\n",
      "65/76 - Running Loss: 0.5577\n",
      "70/76 - Running Loss: 0.4032\n",
      "75/76 - Running Loss: 0.4999\n",
      "12/20 - Train Loss: 0.0066\n",
      "0/76 - Running Loss: 0.3304\n",
      "5/76 - Running Loss: 0.2964\n",
      "10/76 - Running Loss: 0.2405\n",
      "15/76 - Running Loss: 0.3029\n",
      "20/76 - Running Loss: 0.3325\n",
      "25/76 - Running Loss: 0.3901\n",
      "30/76 - Running Loss: 0.3318\n",
      "35/76 - Running Loss: 0.3628\n",
      "40/76 - Running Loss: 0.3368\n",
      "45/76 - Running Loss: 0.3919\n",
      "50/76 - Running Loss: 0.4042\n",
      "55/76 - Running Loss: 0.2701\n",
      "60/76 - Running Loss: 0.4045\n",
      "65/76 - Running Loss: 0.4083\n",
      "70/76 - Running Loss: 0.5090\n",
      "75/76 - Running Loss: 0.3013\n",
      "13/20 - Train Loss: 0.0040\n",
      "0/76 - Running Loss: 0.2165\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_cell_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs = EPOCHS\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbest_epoch = 0\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbest_score = 0.0\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtrain_loss = []\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# val_loss, val_dice_coefficient = [], []\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfor epoch in range(num_epochs):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    losses = train_one_epoch(trainloader, model, optimizer, criterion, DEVICE)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    train_loss.append(losses[\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m])\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    # val_loss.append(losses[\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m])\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    print(f\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{epoch}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{num_epochs}\u001b[39;00m\u001b[38;5;124m - Train Loss: \u001b[39m\u001b[38;5;132;01m{losses[\\'train\\']:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    if epoch \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m 3 ==0:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        save_model(model.state_dict(), f\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{epoch:02d}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mprint(f\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest epoch: \u001b[39m\u001b[38;5;132;01m{best_epoch}\u001b[39;00m\u001b[38;5;124m -> Best Dice Coeffient: \u001b[39m\u001b[38;5;132;01m{best_score:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unet\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2475\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2473\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m   2474\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[1;32m-> 2475\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2477\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[0;32m   2478\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unet\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:1170\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m   1169\u001b[0m     number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m index\n\u001b[1;32m-> 1170\u001b[0m     time_number \u001b[38;5;241m=\u001b[39m timer\u001b[38;5;241m.\u001b[39mtimeit(number)\n\u001b[0;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[0;32m   1172\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unet\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:158\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    156\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner(it, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[1;32m<magic-timeit>:9\u001b[0m, in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "Cell \u001b[1;32mIn[73], line 24\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(trainloader, model, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# optimizer의 갱신할 기울기 값을, 즉, 연결되어있는 모든 model params의 기울기값들을 초기화\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     27\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unet\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unet\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "num_epochs = EPOCHS\n",
    "\n",
    "best_epoch = 0\n",
    "best_score = 0.0\n",
    "train_loss = []\n",
    "# val_loss, val_dice_coefficient = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = train_one_epoch(trainloader, model, optimizer, criterion, DEVICE)\n",
    "    train_loss.append(losses[\"train\"])\n",
    "    # val_loss.append(losses[\"val\"])\n",
    "    \n",
    "    print(f\"{epoch}/{num_epochs} - Train Loss: {losses['train']:.4f}\")\n",
    "    \n",
    "    if epoch % 3 ==0:\n",
    "        save_model(model.state_dict(), f\"model_{epoch:02d}.pth\")\n",
    "        \n",
    "print(f\"Best epoch: {best_epoch} -> Best Dice Coeffient: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88837590-f157-4479-9ef0-08ec7e27a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. weight 저장후, inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee27ead-84fa-4902-a91a-095189fd055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9. 평가지표로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807919ee-3355-4cf2-a1f9-cf8e98ed2bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f9a62-2684-4cb7-963e-1e49a4674a25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "unet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
